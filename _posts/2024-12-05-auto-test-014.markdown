---
layout:     post
title:      "Web Client-Side Performance Testing Automation Solution"
subtitle:   "「 End to End Auto Testing 」" 
date:       2024-12-05 12:00:00
author:     "Vick Huang"
header-img: "img/bg-walle.jpg"
catalog: true
tags:
    - Testing
    - Automated Testing
---


# **Web Client Performance Testing Automation Solution**

---

## **Part 1: Overview and Core Design**

---

### **1. Background**

In modern web application development, performance testing is crucial for ensuring quality and stability. Beyond response time testing, monitoring JavaScript memory usage is essential for addressing issues like memory leaks or performance bottlenecks. This solution provides a comprehensive testing framework that integrates automatic testing, memory monitoring, and detailed report generation.

---

### **2. Objectives**

| Objective                           | Description                                                   |
|-------------------------------------|---------------------------------------------------------------|
| **Modular Design**                  | Allow dynamic extensions of test points for scalability.      |
| **Response Time**               | Captures and analyzes response times,including P90, P95, P99   |
| **Memory Monitoring**               | Capture and monitor JavaScript memory usage per test cycle.   |
| **Comprehensive Reporting**         | Generate visualized reports (charts, tables) and save results.|
| **Data Persistence**                | Save all performance data to CSV files for further analysis.  |

---

### **3. Technology Stack**

| Technology        | Purpose                                |
|--------------------|----------------------------------------|
| **Python**         | Core framework implementation         |
| **Selenium**       | Web UI automation                    |
| **pandas**         | Data analysis and aggregation         |
| **matplotlib**     | Report visualization (charts/tables)  |
| **Robot Framework**| Test case management and execution    |

---

### **4. System Architecture**

| Module/File Name                  | Description                                                   |
|-----------------------------------|---------------------------------------------------------------|
| **`base_function.py`**            | Abstract class for defining test points.                     |
| **`performance_tester.py`**       | Core class for managing test execution and reporting.         |
| **`robot_framework_wrapper.py`** | Robot Framework library wrapper for tester integration.       |
| **`test_functions.py`**           | Implementation of specific test cases.                       |
| **`web_performance_test.robot`**  | Robot Framework test case file.                              |

---

### **5. Abstract Base Class**

`base_function.py` is the foundation for defining test points. It abstracts common procedures like preconditions, operations, and postconditions.

#### **Code Example**

```python
from abc import ABC, abstractmethod
import time


class TestFunctionBase(ABC):
    def __init__(self, tester):
        self.tester = tester

    @abstractmethod
    def precondition(self):
        pass

    @abstractmethod
    def measure_operation(self):
        pass

    @abstractmethod
    def postcondition(self):
        pass

    def run(self):
        try:
            self.precondition()
            start_time = time.time()
            self.measure_operation()
            end_time = time.time()
            response_time = round(end_time - start_time, 2)
            self.postcondition()
            return response_time
        except Exception as e:
            print(f"Error in {self.__class__.__name__}: {e}")
            return None
```

---

### **6. Core Testing Class**

`performance_tester.py` manages the following:
- **Test execution**: Iteratively runs test points and captures results (response time, memory usage).
- **Visualization**: Generates charts and tables for performance analysis.
- **Data persistence**: Saves results to CSV files.

#### **Key Functionalities**

| Feature                          | Description                                                   |
|----------------------------------|---------------------------------------------------------------|
| **`execute_tests`**              | Runs all registered test points for a specified number of rounds.|
| **`generate_memory_usage_chart`**| Creates a trend chart for JavaScript memory usage.            |
| **`generate_response_time`**     | Creates response time bar charts and trend charts.            |
| **`save_to_csv`**                | Saves response time and memory usage data to CSV files.       |

#### **Core Code**

```python
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
import base64


class WebPerformanceTester:
    def __init__(self, driver, target_url, rounds=5):
        self.driver = driver
        self.target_url = target_url
        self.rounds = rounds
        self.response_time_data = []
        self.memory_usage_data = []

    def get_js_memory(self):
        try:
            js_memory = self.driver.execute_script("return window.performance.memory;")
            if js_memory:
                used_js_memory_mb = js_memory.get("usedJSHeapSize", 0) / (1024 * 1024)
                return round(used_js_memory_mb, 2)
        except Exception as e:
            print(f"Error in get_js_memory: {e}")
        return None

    def execute_tests(self, test_functions):
        for round_num in range(self.rounds):
            self.driver.get(self.target_url)
            memory_usage = self.get_js_memory()
            if memory_usage is not None:
                self.memory_usage_data.append({"round": round_num + 1, "used_MB": memory_usage})

            for func in test_functions:
                response_time = func.run()
                self.response_time_data.append({
                    "round": round_num + 1,
                    "function_point": func.__class__.__name__,
                    "response_time": response_time
                })

    def generate_memory_usage_chart(self):
        df = pd.DataFrame(self.memory_usage_data)
        plt.figure(figsize=(10, 6))
        plt.plot(df["round"], df["used_MB"], marker="o", label="Used Memory (MB)", color="blue")
        plt.title("JavaScript Memory Usage Trend")
        plt.xlabel("Round")
        plt.ylabel("Memory (MB)")
        plt.grid()
        plt.legend()
        return self._save_fig_as_base64()

    def generate_response_time_statistics_chart(self):
        df = pd.DataFrame(self.response_time_data)
        stats = df.groupby("function_point").agg({"response_time": ["mean", "max"]})
        stats.columns = ["Mean", "Max"]
        stats.reset_index(inplace=True)

        stats.plot(kind="bar", x="function_point", figsize=(10, 6), colormap="coolwarm")
        plt.title("Response Time Statistics")
        plt.xlabel("Function Point")
        plt.ylabel("Time (s)")
        plt.grid(axis="y")
        plt.xticks(rotation=45)
        return self._save_fig_as_base64()

    def generate_response_time_trend_chart(self):
        df = pd.DataFrame(self.response_time_data)
        plt.figure(figsize=(10, 6))
        for func in df["function_point"].unique():
            func_data = df[df["function_point"] == func]
            plt.plot(func_data["round"], func_data["response_time"], marker="o", label=func)

        plt.title("Response Time Trend")
        plt.xlabel("Round")
        plt.ylabel("Response Time (s)")
        plt.legend(title="Function Points")
        plt.grid()
        return self._save_fig_as_base64()

    def generate_response_time_statistics_table(self):
        """Generate a response time statistics table as an image."""
        df = pd.DataFrame(self.response_time_data)
        stats = df.groupby("function_point").agg({
            "response_time": ["mean", "max", "min"]
        }).reset_index()
        stats.columns = ["Function Point", "Mean (s)", "Max (s)", "Min (s)"]

        fig, ax = plt.subplots(figsize=(8, len(stats) * 0.8))
        ax.axis("tight")
        ax.axis("off")
        table = plt.table(
            cellText=stats.values,
            colLabels=stats.columns,
            cellLoc="center",
            loc="center"
        )
        table.auto_set_font_size(False)
        table.set_fontsize(8)
        return self._save_fig_as_base64()

    def save_to_csv(self):
        if self.response_time_data:
            response_time_df = pd.DataFrame(self.response_time_data)
            response_time_df.to_csv("response_time_data.csv", index=False)
            print("Response time data saved to 'response_time_data.csv'.")

        if self.memory_usage_data:
            memory_usage_df = pd.DataFrame(self.memory_usage_data)
            memory_usage_df.to_csv("memory_usage_data.csv", index=False)
            print("Memory usage data saved to 'memory_usage_data.csv'.")

    def _save_fig_as_base64(self):
        buf = BytesIO()
        plt.savefig(buf, format="png", bbox_inches="tight")
        buf.seek(0)
        base64_image = base64.b64encode(buf.getvalue()).decode("utf-8")
        plt.close()
        return base64_image
```

---

## **Part 2: Implementation and Execution**

---

### **7. Robot Framework Integration**

`robot_framework_wrapper.py` is a custom library for integrating the `WebPerformanceTester` class with **Robot Framework**. It simplifies the test flow by exposing keywords like initialization, report generation, and test execution.

#### **Code**

```python
from robot.api import logger
from performance_tester import WebPerformanceTester
from selenium import webdriver


class RobotFrameworkWebTester:
    def __init__(self):
        self.tester = None

    def initialize_tester(self, driver_path, target_url, rounds=5):
        """Initialize the performance tester."""
        driver = webdriver.Chrome(executable_path=driver_path)
        self.tester = WebPerformanceTester(driver, target_url, int(rounds))
        logger.info("Tester initialized successfully.")

    def execute_tests(self, module_name, class_names):
        """Dynamically register and execute test functions."""
        from importlib import import_module

        if not self.tester:
            raise ValueError("Tester is not initialized.")

        test_functions = []
        module = import_module(module_name)
        for class_name in class_names:
            cls = getattr(module, class_name, None)
            if cls is None:
                raise ValueError(f"Class '{class_name}' not found in module '{module_name}'.")
            test_functions.append(cls(self.tester))

        self.tester.execute_tests(test_functions)

    def generate_reports(self):
        """Generate performance reports (charts and tables)."""
        if not self.tester:
            raise ValueError("Tester is not initialized.")

        memory_chart = self.tester.generate_memory_usage_chart()
        response_time_stats_chart = self.tester.generate_response_time_statistics_chart()
        response_time_trend_chart = self.tester.generate_response_time_trend_chart()
        response_time_table = self.tester.generate_response_time_statistics_table()

        logger.info('<h2>Memory Usage Trend Chart</h2>', html=True)
        logger.info(f'<img src="data:image/png;base64,{memory_chart}" alt="Memory Usage Trend"/>', html=True)

        logger.info('<h2>Response Time Statistics Chart</h2>', html=True)
        logger.info(f'<img src="data:image/png;base64,{response_time_stats_chart}" alt="Response Time Statistics"/>', html=True)

        logger.info('<h2>Response Time Trend Chart</h2>', html=True)
        logger.info(f'<img src="data:image/png;base64,{response_time_trend_chart}" alt="Response Time Trend"/>', html=True)

        logger.info('<h2>Response Time Statistics Table</h2>', html=True)
        logger.info(f'<img src="data:image/png;base64,{response_time_table}" alt="Response Time Statistics Table"/>', html=True)

    def save_to_csv(self):
        """Save test data (response time and memory usage) to CSV files."""
        if self.tester:
            self.tester.save_to_csv()

    def quit_tester(self):
        """Quit WebDriver instance."""
        if self.tester:
            self.tester.driver.quit()
            logger.info("Tester quit successfully.")
```

---

### **8. Specific Test Point Implementation**

`test_functions.py` defines the test points by inheriting `TestFunctionBase`. Each test point specifies preconditions, operations, and postconditions.

#### **Code**

```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from base_function import TestFunctionBase


class TestFunction1(TestFunctionBase):
    def precondition(self):
        """Ensure the page and button are ready."""
        WebDriverWait(self.tester.driver, 10).until(
            EC.presence_of_element_located((By.ID, "button1"))
        )

    def measure_operation(self):
        """Click a button."""
        WebDriverWait(self.tester.driver, 5).until(
            EC.element_to_be_clickable((By.ID, "button1"))
        ).click()

    def postcondition(self):
        """No specific cleanup required."""
        pass


class TestFunction2(TestFunctionBase):
    def precondition(self):
        """Ensure input field is ready."""
        WebDriverWait(self.tester.driver, 10).until(
            EC.presence_of_element_located((By.ID, "input2"))
        )

    def measure_operation(self):
        """Enter data and submit."""
        input_field = WebDriverWait(self.tester.driver, 5).until(
            EC.presence_of_element_located((By.ID, "input2"))
        )
        input_field.send_keys("Testing")
        WebDriverWait(self.tester.driver, 5).until(
            EC.element_to_be_clickable((By.ID, "submit2"))
        ).click()

    def postcondition(self):
        """Clear the input field."""
        input_field = self.tester.driver.find_element(By.ID, "input2")
        input_field.clear()
```

---

### **9. Robot Framework Test Case**

`web_performance_test.robot` defines the end-to-end performance testing workflow using **Robot Framework**.

#### **Code**

```robot
*** Settings ***
Library    robot_framework_wrapper.py

*** Variables ***
${CHROME_DRIVER_PATH}    /path/to/chromedriver
${TARGET_URL}            https://example.com
${ROUNDS}               5
${MODULE_NAME}          test_functions
@{CLASS_NAMES}          TestFunction1    TestFunction2

*** Test Cases ***
Web Client Performance Test
    Initialize Tester    ${CHROME_DRIVER_PATH}    ${TARGET_URL}    ${ROUNDS}
    Execute Tests        ${MODULE_NAME}          @{CLASS_NAMES}
    Generate Reports
    Save to CSV
    Quit Tester
```

---

### **10. Execution Steps**

#### **10.1 Preparation**

1. Install dependencies:
   ```bash
   pip install selenium robotframework pandas matplotlib
   ```

2. Download **ChromeDriver** (ensure it matches your browser version) and set its path in `${CHROME_DRIVER_PATH}`.

3. Adjust `${TARGET_URL}` to point to your test page, e.g., `https://example.com`.

---

#### **10.2 Run the Test**

Execute the Robot Framework test case:
```bash
robot web_performance_test.robot
```

---

### **11. Output**

#### **11.1 Charts and Tables**

- **Memory Usage Trend Chart**: Tracks JavaScript heap memory across test rounds.
- **Response Time Statistics Chart**: Bar chart showing mean and max response time for each test point.
- **Response Time Trend Chart**: Line chart tracking response time across test rounds.
- **Response Time Statistics Table**: Displays mean, max, and min response time for each test point (embedded as an image).

#### **11.2 CSV Files**

1. **Response Time Data (`response_time_data.csv`)**:
   ```csv
   round,function_point,response_time
   1,TestFunction1,0.45
   1,TestFunction2,0.30
   2,TestFunction1,0.47
   ```

2. **Memory Usage Data (`memory_usage_data.csv`)**:
   ```csv
   round,used_MB
   1,50.23
   2,51.10
   3,49.85
   ```

---

### **12. Advantages**

| Feature                           | Benefit                                                     |
|-----------------------------------|-------------------------------------------------------------|
| **Modular Design**                | Easily extendable with new test points.                    |
| **Comprehensive Reporting**       | Generates charts, tables, and data files for analysis.      |
| **Robot Framework Integration**   | Simplifies automation and execution.                       |
| **Multi-Dimensional Analysis**    | Covers both response time and JavaScript memory usage.      |

---

With this comprehensive solution, you can efficiently automate and analyze web client performance, identify bottlenecks, and optimize your web applications.