---
layout:     post
title:      "企业级软件测试工程——知识分层体系"
subtitle:   "「  AI Study 」" 
date:       2025-12-06 11:00:01
author:     "Vick Huang"
header-img: "img/bg-walle.jpg"
catalog: true
tags:
    - AI
---

# 企业级软件测试工程：知识分层体系与 AI Prompt 最佳实践指南
**（含 Chroma + Python RAG 实现方案，按需求与代码完整重写版）**

---

## 摘要

在现代企业的软件测试体系中，**知识是核心生产资料**：
从测试方法论、行业模型，到具体项目的需求文档、接口说明，再到历史缺陷与线上事故，这些信息如果没有清晰的结构化管理，很容易导致：

- 知识分散在 Wiki、Jira、代码、个人经验中，难以检索与复用
- 同类功能在不同项目重复造轮子，测试资产无法积累
- 用例风格不统一，质量参差不齐，难以自动化
- 需求变更频繁，文档快速过期，与实际系统脱节
- 历史缺陷没有沉淀，新项目不断踩同样的坑

本方案从 **“企业级知识分层 + AI Prompt 标准 + 可执行 RAG 技术实现”** 三个维度出发，提出：

1. 一个 **可治理、可复用、可演进** 的四层测试知识体系（L1–L4）
2. 一套 **可直接用于生产环境** 的 AI 测试用例生成 Prompt 标准
3. 基于 **Chroma + Python** 的完整 RAG 实现代码（包含：
   - 文本切分（Chunking）
   - 向量化与存储
   - 版本管理
   - 多模型调用封装
   - 用例生成流程示例）

本文可作为企业内部正式方案文档，指导质量平台团队与测试团队落地 “**AI 驱动的软件测试工程**”。

---

# 1. 背景与问题分析

## 1.1 典型现状

在多数企业中，测试知识管理呈现如下特征：

- **强依赖个人**：
  关键测试场景依赖“老员工记忆”，新人很难快速接手。
- **知识高度分散**：
  规范在 Confluence，需求在蓝湖/墨刀，接口在 Swagger，历史缺陷在 Jira 或 Excel，分散在多个系统，靠人脑做“RAG”。
- **重复建设严重**：
  相似的登录流程、支付流程、审批流程，各项目组分别建自己的用例与脚本。
- **文档快速过期**：
  需求变更后，很少同步更新测试文档；用例与实际系统严重偏离。
- **无法利用历史经验**：
  线上事故与严重缺陷复盘只停留在邮件/会议纪要中，很少结构化沉淀到知识库。

在此背景下，引入大模型（LLM）如果没有良好的知识管理，很容易变成：

> “AI 生成一堆看起来很高级，但和当前项目不太相关的用例。”

而不是企业真正需要的：

> “**结合企业标准 + 行业逻辑 + 当前项目上下文 + 历史经验，自动生成高质量测试用例**。”

## 1.2 方案目标

本方案的核心目标：

1. **构建清晰分层的企业级测试知识体系**
2. 为企业搭建 **可扩展的 RAG 知识库（Chroma + Python）**
3. 提供 **标准化、可复用的 AI 测试用例生成 Prompt**
4. 降低项目之间的知识割裂，让经验变成可复用资产
5. 实现：
   - 提升测试覆盖率
   - 提升用例质量与一致性
   - 提升自动化可行性与效率

**非目标：**

- 不试图替代所有测试设计工作，而是重点提升 **重复性与标准化场景** 的效率
- 不限制企业具体选用哪家模型服务商（OpenAI / Azure / Gemini / 自研皆可）

---

# 2. 总体方案概览

总体设计可以概括为：

> **“L1–L4 知识分层 + RAG 架构 + 标准 Prompt + 代码实现”**

## 2.1 四层测试知识体系（L1–L4）

按“稳定性 + 复用范围”分层：

- **L1：方法论与规范层**（企业统一、不易变）
- **L2：领域与业务逻辑层**（行业/平台层，跨项目复用）
- **L3：项目上下文层**（某个项目版本特有，高频变更）
- **L4：资产与历史层**（随着时间不断累积的缺陷与经验）

## 2.2 RAG 技术架构（简要）

1. **离线阶段（知识构建）**
   - 将 L1–L4 的文档整理为 Markdown / YAML / JSON
   - 使用 Chunker 按语义切分为自然段
   - 使用向量模型（如 text-embedding-3-large）转换为向量
   - 存入 Chroma，写入层级、版本等元数据

2. **在线阶段（测试用例生成）**
   - 测试人员输入：某功能/接口说明 + 期望条数
   - 系统从 Chroma 中分别检索：
     - L1 标准和方法论
     - L2 领域规则
     - L3 项目上下文
     - L4 历史缺陷
   - 将检索结果 + 标准 Prompt 提供给大模型
   - 模型生成结构化测试用例列表（可选 JSON 输出）

3. **闭环（持续优化）**
   - 将新增缺陷、高价值用例、线上事故复盘导入 L4
   - 定期治理过期的 L3 项目文档

---

# 3. 企业级测试知识分层体系（L1–L4）

本节重点说明每一层的 **定位、内容范畴、示例与落地建议**。

---

## 3.1 L1：方法论与规范层（Methodology & Standards）

### 3.1.1 性质

- 企业级统一
- 极稳定，更新频率低
- 全项目通用

### 3.1.2 典型内容

- **测试方法论**
  - 边界值分析、等价类、决策表、因果图、场景法、错误推测
- **测试规范**
  - 用例命名规范（格式、语言、命名风格）
  - 用例优先级定义（P0–P3 含判定标准）
  - 缺陷严重级别（Blocker/Critical/Major/Minor）
- **自动化标准**
  - pytest 编码约定、目录结构
  - Page Object 模型规范
  - 日志规范、断言规范
- **安全与合规规范**
  - 个人隐私信息（PII）处理
  - 数据脱敏规则

### 3.1.3 建议存储形式

- Markdown 文档，结构清晰，如：

```markdown
# L1_测试方法论与规范

## 1. 测试用例命名规范
- 格式：`[模块]_[功能]_[场景]_[预期结果]`
- 语言：建议统一中文或中英混合，但同一项目保持一致
...

## 2. 用例优先级定义（P0–P3）
- P0：核心交易路径，失败造成重大业务中断
- P1：主要功能路径，失败造成显著体验或效率问题
...

## 3. 测试设计方法
### 3.1 边界值
说明 + 示例

### 3.2 等价类
说明 + 示例
...
```

---

## 3.2 L2：领域与业务逻辑层（Domain & Business Logic）

### 3.2.1 性质

- 针对某一行业或平台
- 跨项目复用
- 更新缓慢（除非法规或核心业务调整）

### 3.2.2 典型内容

- **行业知识**
  - 金融：授信规则、还款逻辑、逾期处理、利率计算方式
  - 电商：订单生命周期（创建-支付-发货-收货-退货）、优惠券规则
  - 医疗：就诊流程、处方合规要求、数据留存要求
- **通用业务模型**
  - 用户状态机（未注册/已注册/冻结/注销）
  - 订单状态机（待支付/已支付/已发货/已签收/已取消）
  - 对账流程（T+1 清算、差异处理）
- **全局数据字典**
  - 错误码定义（如 `E10001：用户未登录`）
  - 状态字段含义与取值约束
  - 事件类型（创建/修改/取消/重试等）

### 3.2.3 建议存储形式

- 多文档：
  - 业务流程用 Markdown + 流程图（Mermaid）
  - 字典类信息可用 YAML/JSON，便于自动校验

示例（YAML）：

```yaml
# L2_domain/order_status.yaml
order_status:
  - code: CREATED
    description: 订单已创建，未支付
  - code: PAID
    description: 已支付，待发货
  - code: SHIPPED
    description: 已发货，待收货
  - code: COMPLETED
    description: 用户确认收货
  - code: CANCELED
    description: 用户取消订单
```

---

## 3.3 L3：项目上下文层（Project Context）

### 3.3.1 性质

- **某一个项目** 专有
- 与版本高度相关，变更频繁
- AI 用例生成时的 **主检索来源**

### 3.3.2 典型内容

- PRD、详细需求文档、原型图
- API 文档（Swagger、OpenAPI）、参数说明
- 数据库 Schema（表字段约束）
- 模块拆分、服务拓扑图
- 该项目的变更日志、版本说明

### 3.3.3 版本化与结构建议

- 目录结构示例：

```bash
/knowledge
  /L3_project_X
      /version_1.0.0
          prd_login.md
          api_auth.yaml
          db_user.yaml
      /version_1.1.0
          prd_login.md
          api_auth.yaml
```

- 各版本之间可共享相同文件（软链接或独立 copy），但语义上按版本隔离，便于回溯。

---

## 3.4 L4：资产与历史层（Assets & History）

### 3.4.1 性质

- 持续积累
- 重点沉淀 “踩过的坑”和 “高价值测试资产”

### 3.4.2 典型内容

- 历史缺陷库（含缺陷描述、根因、影响范围、修复方案）
- 冒烟用例集、回归用例集（手工 + 自动化）
- 线上事故复盘（含时间线、根因、预防措施）
- 用户投诉与反馈（结构化分类：功能/性能/稳定性/安全等）

### 3.4.3 建议结构

- 按系统/模块划分子目录
- 用统一结构描述缺陷，如 JSON：

```json
{
  "bug_id": "PAY-2024-00123",
  "module": "payment",
  "severity": "Critical",
  "summary": "重复扣款",
  "root_cause": "幂等性校验缺失，重试请求重复执行",
  "affected_versions": ["1.2.0", "1.2.1"],
  "fix": "增加幂等 key 校验，记录处理状态",
  "preventive_actions": [
    "核心支付接口增加幂等性相关测试用例",
    "回归测试增加重复请求场景"
  ]
}
```

AI 在生成新项目的支付测试用例时，即可自动参考此类缺陷模式，重点覆盖重复扣款、请求重试等高风险点。

---

# 4. 知识治理原则与目录结构

## 4.1 三大治理原则

### 4.1.1 信息去重

> **同一条知识只在一个层级存一次**

- 例如：订单通用状态机属于 L2，而不是每个项目 L3 再写一份。
- 项目若有特殊状态，可在 L3 中记录“在标准 L2 状态机基础上的差异”。

### 4.1.2 分层引用，避免反向依赖

- L3（项目）可以引用 L1/L2 的内容；
- 但 **不得直接修改 L1/L2**，否则会影响所有项目。
- 修订 L1/L2 应经过评审流程，且版本化管理。

### 4.1.3 可版本化

- L3 与 L4 必须支持版本快照：
  - PRD、API 文档随版本变更
  - 重要缺陷可关联发生版本、修复版本
- 建议规范命名：

```text
project_X_version_1.2.0
project_X_version_1.2.1
```

## 4.2 推荐目录结构（面向 RAG）

```bash
/knowledge
  /L1_standards          # 方法论 & 规范
      testing_methodology.md
      testcase_style.md
      automation_guideline.md

  /L2_domain             # 行业与业务逻辑
      /finance
          credit_rules.md
          repay_flow.md
      /ecommerce
          order_lifecycle.md
          coupon_rules.yaml

  /L3_project_X          # 项目 X
      /version_1.0.0
          prd.md
          api.yaml
          db_schema.yaml

      /version_1.1.0
          prd.md
          api.yaml

  /L4_history            # 历史资产与缺陷
      /incidents
          2024_q1_outage_login.md
      /bugs
          PAY-2024-00123.json
      /regression_sets
          smoke_testset_v1.0.json
```

---

# 5. AI 测试用例生成 Prompt 标准（企业版）

本节给出 **企业级标准 Prompt 模板**，用于结合 L1–L4 知识库进行自动化测试用例生成。

## 5.1 标准 Prompt 模板（RAG 专用）

```text
你是一名企业级软件测试专家（10 年经验），同时遵循公司的四层知识体系：
- L1：方法论与测试规范
- L2：领域与业务逻辑
- L3：当前项目上下文（需求、接口、数据结构）
- L4：历史缺陷与测试经验

【任务目标】
请基于已检索到的相关知识，为给定的功能或接口生成高质量的测试用例集合。

【必须遵循的规则】
1. 用例结构统一，包含字段：
   - 用例编号
   - 用例标题
   - 前置条件
   - 输入数据
   - 触发步骤
   - 预期结果
   - 优先级（P0-P3）
   - 关联需求 ID（如有）
   - 自动化可行性（是/否，简要说明原因）

2. 生成用例时，需要综合考虑：
   - L1 方法论：边界值、等价类、决策表/判定表、错误推测、场景分析
   - L2 业务逻辑：行业规则、错误码规范、状态机转换、业务约束
   - L3 项目上下文：PRD 描述、接口字段要求、参数校验规则、数据库约束等
   - L4 历史经验：已知风险点、历史缺陷模式、线上事故中暴露的问题

3. 覆盖范围要求：
   - 覆盖所有主流程（核心成功路径）
   - 覆盖异常流程（参数缺失、类型错误、权限不足、超时等）
   - 覆盖边界场景（最小值、最大值、临界状态、空值等）
   - 覆盖安全与权限相关场景（未登录、越权访问、敏感数据泄露等）
   - 特别关注历史高风险点（如幂等性、并发、重试、金额精度等）

4. 输出要求：
   - 用例必须可执行、步骤清晰、预期结果可验证
   - 风格统一、逻辑清楚，便于导入测试管理平台
   - 如果可能，请优先标注出适合自动化回归的用例（自动化可行性=是）

【输入内容】
<<PROJECT_CONTEXT>>

【输出格式】
请以结构化列表返回所有测试用例，数量不少于 <<MIN_TESTCASE_COUNT>> 条。
如果可能，请使用类似 JSON 的结构化文本，便于解析。
```

在 RAG 实现中，`<<PROJECT_CONTEXT>>` 通常会动态填充：

- 当前项目/接口的描述
- 从 Chroma 检索到的 L1/L2/L3/L4 相关片段（拼接或摘要）

---

# 6. RAG 技术实现：Chroma + Python 完整方案

本节给出 **可执行代码示例**，包括：

1. 高级 Chunker：多语言 + token 控制
2. Chroma 知识库存储与版本管理
3. 通用多模型 Client（OpenAI / Azure / Gemini）
4. 用例生成流程示例（RAG + Prompt）

> 说明：代码以示例为主，不依赖具体模型服务商，企业可根据实际 SDK 与秘钥配置进行适配。

---

## 6.1 文本切分模块（AdvancedChunker）

目标：

- 保证每个 chunk 具备完整语义（不拆散关键业务逻辑段落）
- 控制每个 chunk 的 token 数量（500–1500 左右）
- 支持中英混合文本

```python
# chunker.py
import re
from typing import List
import tiktoken


class AdvancedChunker:
    """
    面向 RAG 的多语言文本切分器：
    - 以句子为基本单位（中英文标点）
    - 控制 chunk 的 token 上限和下限
    """

    # 句子分隔符：中文句号 / 问号 / 感叹号 + 英文 . ? !
    SENTENCE_SEPARATORS = r"[。！？!?\.]"

    def __init__(self, model: str = "gpt-4o-mini",
                 max_tokens: int = 1200,
                 min_tokens: int = 200):
        """
        :param model: 用于估算 token 的模型名称（tiktoken 使用）
        :param max_tokens: 每个 chunk 最大 token 数
        :param min_tokens: 每个 chunk 最小 token 数（不强制）
        """
        self.enc = tiktoken.encoding_for_model(model)
        self.max_tokens = max_tokens
        self.min_tokens = min_tokens

    def count_tokens(self, text: str) -> int:
        """估算文本 token 数"""
        if not text:
            return 0
        return len(self.enc.encode(text))

    def split_sentences(self, text: str) -> List[str]:
        """按中英文标点拆分为句子列表"""
        parts = re.split(f"({self.SENTENCE_SEPARATORS})", text)
        sentences = []
        for i in range(0, len(parts) - 1, 2):
            sentences.append(parts[i] + parts[i + 1])
        # 去除空白
        return [s.strip() for s in sentences if s.strip()]

    def chunk(self, text: str) -> List[str]:
        """
        核心切分逻辑：
        - 按句子缓慢累积
        - 超过 max_tokens 则换新 chunk
        """
        sentences = self.split_sentences(text)
        chunks = []
        buffer = ""

        for s in sentences:
            candidate = buffer + (" " if buffer else "") + s
            if self.count_tokens(candidate) <= self.max_tokens:
                buffer = candidate
            else:
                # 当前 buffer 已达到上限，先收集为一个 chunk
                if buffer.strip():
                    chunks.append(buffer.strip())
                # 新 chunk 从当前句子开始
                buffer = s

        if buffer.strip():
            chunks.append(buffer.strip())

        return chunks
```

---

## 6.2 Chroma 知识库封装与版本管理

> 说明：下面以 `chromadb` 为示例，你可以根据实际版本使用 `chromadb.Client` 或 `chromadb.PersistentClient`。

```python
# knowledge_base.py
from typing import Dict, Any, List, Optional
import chromadb
from chromadb.config import Settings


class KnowledgeBase:
    """
    基于 Chroma 的企业级知识库封装：
    - 支持基础 CRUD
    - 支持按项目与版本管理文档
    - 通过 metadata 管理 L1/L2/L3/L4、project、version 等信息
    """

    def __init__(self, persist_path: str = "chroma_store"):
        # 使用 DuckDB + Parquet 持久化
        self.client = chromadb.Client(
            Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=persist_path
            )
        )
        self.collections: Dict[str, chromadb.api.models.Collection.Collection] = {}

    def get_collection(self, name: str):
        """
        根据名称获取或创建 Collection。
        建议按“知识层级 + 项目粒度”拆分，如：
        - L1_standards
        - L2_domain
        - L3_project_X
        - L4_history
        """
        if name not in self.collections:
            self.collections[name] = self.client.get_or_create_collection(name=name)
        return self.collections[name]

    # ================ 基础 CRUD ===================

    def add_document(
        self,
        collection: str,
        doc_id: str,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        向指定 collection 写入一条文档（一个 chunk）
        :param collection: 集合名
        :param doc_id: 文档唯一标识（建议 project_version_xxx 格式）
        :param text: 文本内容
        :param metadata: 额外元数据（层级、版本、模块等）
        """
        coll = self.get_collection(collection)
        coll.add(
            documents=[text],
            ids=[doc_id],
            metadatas=[metadata or {}]
        )

    def query(
        self,
        collection: str,
        query_text: str,
        top_k: int = 5,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        在指定 collection 中进行向量检索
        :param query_text: 查询文本
        :param top_k: 返回结果数
        :param where: 过滤条件，如 {"layer": "L3", "project": "X", "version": "1.1.0"}
        """
        coll = self.get_collection(collection)
        return coll.query(
            query_texts=[query_text],
            n_results=top_k,
            where=where
        )

    def delete(self, collection: str, doc_ids: List[str]):
        coll = self.get_collection(collection)
        coll.delete(ids=doc_ids)

    def update_document(
        self,
        collection: str,
        doc_id: str,
        new_text: str,
        new_metadata: Optional[Dict[str, Any]] = None,
    ):
        # 简单实现：先删再加
        self.delete(collection, [doc_id])
        self.add_document(collection, doc_id, new_text, new_metadata)

    # ================ 版本化存储封装 ===================

    def add_versioned_document(
        self,
        collection: str,
        layer: str,
        project: Optional[str],
        version: Optional[str],
        doc_id: str,
        text: str,
        extra_metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        规范化的版本化写入：
        - metadata 自动带上 layer / project / version
        - doc_id 可根据 project/version 做前缀
        """
        full_id = doc_id
        if project and version:
            full_id = f"{project}_{version}_{doc_id}"

        metadata = {
            "layer": layer,          # L1/L2/L3/L4
            "project": project,
            "version": version,
        }
        if extra_metadata:
            metadata.update(extra_metadata)

        self.add_document(collection, full_id, text, metadata)
```

---

## 6.3 多模型 Client（OpenAI / Azure / Gemini 等）

实际生产中，企业可能会：

- 部分场景用 Azure OpenAI
- 部分用本地大模型
- 部分用 Google Gemini

因此需要一个统一封装层。

```python
# llm_client.py
from typing import Dict, List, Generator, Any, Optional
import json


class MultiModelClient:
    """
    多模型统一封装：
    - 支持 Azure OpenAI / OpenAI / Gemini（示例）
    - 支持流式输出
    - 支持简单的 “structured output” 模式
    """

    def __init__(self, provider: str = "openai", **kwargs):
        """
        :param provider: "openai" | "azure" | "gemini" | "other"
        :param kwargs: 各模型所需的配置，例如 api_key, base_url, model 等
        """
        self.provider = provider
        self.kwargs = kwargs

    def stream_chat(self, messages: List[Dict[str, str]]) -> Generator[Dict[str, Any], None, None]:
        """
        统一流式输出接口
        """
        provider = self.provider

        if provider == "azure":
            from openai import AzureOpenAI

            client = AzureOpenAI(
                api_key=self.kwargs.get("api_key"),
                api_version=self.kwargs.get("api_version"),
                azure_endpoint=self.kwargs.get("azure_endpoint"),
            )
            stream = client.chat.completions.create(
                model=self.kwargs["model"],
                messages=messages,
                stream=True
            )
            for chunk in stream:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    yield {"type": "text", "content": delta.content}

        elif provider == "openai":
            from openai import OpenAI

            client = OpenAI(
                api_key=self.kwargs.get("api_key"),
                base_url=self.kwargs.get("base_url")
            )
            stream = client.chat.completions.create(
                model=self.kwargs["model"],
                messages=messages,
                stream=True
            )
            for chunk in stream:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    yield {"type": "text", "content": delta.content}

        elif provider == "gemini":
            import google.generativeai as genai

            genai.configure(api_key=self.kwargs["api_key"])
            model = genai.GenerativeModel(self.kwargs["model"])
            response = model.generate_content(
                messages[-1]["content"],
                stream=True
            )
            for chunk in response:
                if chunk.text:
                    yield {"type": "text", "content": chunk.text}

        else:
            # 预留给自研模型或其他厂商
            raise NotImplementedError(f"Provider {provider} is not implemented.")

    def generate_text(self, messages: List[Dict[str, str]]) -> str:
        """
        非流式一次性返回完整文本
        """
        text = ""
        for piece in self.stream_chat(messages):
            text += piece["content"]
        return text

    def generate_structured(
        self,
        messages: List[Dict[str, str]],
        try_json: bool = True
    ) -> Dict[str, Any]:
        """
        简易版结构化输出：
        - 优先尝试把模型输出解析为 JSON
        - 解析失败则返回 raw_text
        """
        raw = self.generate_text(messages)

        if try_json:
            # 简单尝试：截取第一个大括号开始到最后一个大括号
            try:
                start = raw.find("{")
                end = raw.rfind("}")
                if start != -1 and end != -1 and end > start:
                    trimmed = raw[start : end + 1]
                    return json.loads(trimmed)
            except Exception:
                pass

        return {"raw_text": raw}
```

---

## 6.4 用例生成服务示例：将 RAG + Prompt 串起来

下面示例展示如何将：

- 知识库（Chroma）
- Prompt 模板
- 多模型 Client

组合，构造一个简单的“用例生成”函数。

```python
# testcase_generator.py
from typing import List, Dict, Any
from knowledge_base import KnowledgeBase
from llm_client import MultiModelClient


class TestcaseGenerator:
    """
    基于 RAG 的企业级测试用例生成服务示例：
    - 从 L1/L2/L3/L4 检索相关知识
    - 组合成上下文
    - 使用标准 Prompt + LLM 生成测试用例
    """

    def __init__(self, kb: KnowledgeBase, llm: MultiModelClient):
        self.kb = kb
        self.llm = llm

    def _retrieve_knowledge(
        self,
        project: str,
        version: str,
        user_query: str,
        top_k: int = 4
    ) -> Dict[str, List[str]]:
        """
        根据用户输入，从 L1~L4 检索相关 chunk
        """
        results = {
            "L1": [],
            "L2": [],
            "L3": [],
            "L4": [],
        }

        # 1. L1：方法论与规范
        r1 = self.kb.query(
            collection="L1_standards",
            query_text=user_query,
            top_k=top_k
        )
        results["L1"] = r1.get("documents", [[]])[0]

        # 2. L2：领域逻辑（这里示例直接查 L2_domain 集合）
        r2 = self.kb.query(
            collection="L2_domain",
            query_text=user_query,
            top_k=top_k
        )
        results["L2"] = r2.get("documents", [[]])[0]

        # 3. L3：项目上下文（按 project + version 过滤）
        r3 = self.kb.query(
            collection="L3_project",
            query_text=user_query,
            top_k=top_k,
            where={"project": project, "version": version}
        )
        results["L3"] = r3.get("documents", [[]])[0]

        # 4. L4：历史资产（可按项目或模块过滤，这里简化）
        r4 = self.kb.query(
            collection="L4_history",
            query_text=user_query,
            top_k=top_k
        )
        results["L4"] = r4.get("documents", [[]])[0]

        return results

    def _build_context_text(
        self,
        knowledge: Dict[str, List[str]]
    ) -> str:
        """
        将检索到的 L1~L4 片段拼接为上下文文本
        注意：在真实系统中可以进一步做摘要或截断
        """
        parts = []
        for layer in ["L1", "L2", "L3", "L4"]:
            docs = knowledge.get(layer, [])
            if not docs:
                continue
            joined = "\n\n---\n".join(docs)
            parts.append(f"【{layer} 相关知识】\n{joined}")

        return "\n\n".join(parts)

    def generate_testcases(
        self,
        project: str,
        version: str,
        feature_description: str,
        min_count: int = 20
    ) -> Dict[str, Any]:
        """
        主流程：生成测试用例
        :param project: 项目名
        :param version: 版本号
        :param feature_description: 功能/接口描述（可以是 PRD 片段或接口说明）
        :param min_count: 期望最少用例数
        :return: 结构化用例或原始文本
        """
        # 1. RAG 检索
        knowledge = self._retrieve_knowledge(
            project=project,
            version=version,
            user_query=feature_description,
            top_k=4
        )
        context_text = self._build_context_text(knowledge)

        # 2. 构造 Prompt
        prompt = f"""
你是一名企业级软件测试专家（10 年经验），遵循公司四层知识体系。
下面是与当前任务相关的企业知识（L1~L4）：

{context_text}

------------------------
【当前待测试的功能/接口描述】
{feature_description}

请严格根据上述知识，生成不少于 {min_count} 条高质量测试用例。
每条用例需包含以下字段：
- 用例编号
- 用例标题
- 前置条件
- 输入数据
- 触发步骤
- 预期结果
- 优先级（P0-P3）
- 关联需求 ID（如有）
- 自动化可行性（是/否）

请尽量以 JSON 数组形式输出，键名使用英文，例如：
[
  {{
    "id": "...",
    "title": "...",
    "preconditions": "...",
    "input": "...",
    "steps": ["...", "..."],
    "expected_result": "...",
    "priority": "P1",
    "requirement_id": "REQ-XXX",
    "automation": "yes"
  }},
  ...
]
"""
        messages = [
            {"role": "system", "content": "你是资深企业级软件测试专家，擅长根据知识库设计系统化测试用例。"},
            {"role": "user", "content": prompt}
        ]

        # 3. 调用大模型，尝试生成结构化 JSON
        result = self.llm.generate_structured(messages)

        return result
```

---

# 7. 落地流程与团队使用方式

## 7.1 知识库建设流程（一次性 + 持续）

1. **一次性建设（1–3 个月）**
   - 由测试架构师牵头，整理 L1 与主要 L2 文档
   - 按项目梳理 L3：从现有需求、接口、数据结构中提取核心内容
   - 收集重大缺陷与线上事故，初步建设 L4

2. **持续维护**
   - **每周**：项目测试负责人清理 L3 当前版本过期内容
   - **每版本发布前后**：
     - 为该版本建立 L3 快照（需求、接口、数据结构）
     - 将本版本高价值回归集与关键缺陷同步到 L4
   - **每月**：Review L1/L2 是否需要更新（规范升级、行业变化）

## 7.2 测试人员日常使用流程

以某项目 X 登录接口测试为例：

1. 测试人员在测试平台中选中：
   - 项目：`project_X`
   - 版本：`1.1.0`
2. 粘贴/选择待测试功能描述，例如：
   > “登录接口：POST /api/v1/login，参数包括 username, password，支持错误码 E10001（用户不存在）、E10002（密码错误）、E10003（账号锁定）……”
3. 设置期望用例条数：如 `30`
4. 系统后端：
   - 调用 `TestcaseGenerator.generate_testcases`
   - 自动检索 L1/L2/L3/L4 相关知识
   - 调用大模型生成用例 JSON
5. 测试人员在平台中审核：
   - 调整个别不合理用例
   - 标记哪些需要落地成自动化脚本
6. 一键导入测试管理系统（如 TestRail、禅道、内建系统）

---

# 8. 运营与效果评估

## 8.1 关键指标（KPI）建议

- **覆盖率提升**：
  - 对比引入方案前后：
    - 核心场景覆盖率
    - 历史缺陷覆盖率（是否有针对复发缺陷的专项用例）
- **效率提升**：
  - 新功能从需求到初版用例集耗时
  - 自动生成用例占比（相对于手工编写）
- **质量提升**：
  - 需求/用例不一致导致的问题数
  - 版本发布后前 N 周线上缺陷率

## 8.2 持续改进建议

- 对模型生成不正确或不适配用例进行标注，反馈给知识库和 Prompt 策略
- 对表现优秀的自动生成用例：
  - 纳入 L4 历史资产
  - 并加入自动化回归集合
- 根据行业/公司具体情况，定期微调：
  - Prompt 中的权重描述（例如增强安全场景或性能场景比重）
  - 检索策略（对 L4 可适当增加权重，以优先考虑历史踩坑点）

---

# 9. 总结与路线图

## 9.1 核心思路总结

1. **知识分层是前提**：
   - 方法论最稳定（L1）
   - 领域逻辑次稳定（L2）
   - 项目上下文多变（L3）
   - 历史经验持续积累（L4）

2. **RAG 是连接知识与 AI 的桥梁**：
   - 在 LLM 前加一层“企业知识检索”，避免空口胡编
   - RAG 的质量高度依赖于：
     - 文档结构化程度
     - Chunking 策略
     - 元数据与过滤策略

3. **Prompt 是“人和知识库之间的协议”**：
   - 明确方法论（L1）、业务逻辑（L2）、项目上下文（L3）、历史资产（L4）的使用方式
   - 标准化用例结构，便于落地与自动化

4. **代码与工程实现必须可执行**：
   - 使用 Chroma + Python 构建可扩展知识库
   - 提供统一大模型调用封装，为后续切换/混用模型打好基础
   - 将用例生成流程封装为服务，嵌入日常测试平台

## 9.2 推荐实施路线图

1. **第 1 阶段：规范与知识梳理（0–1 个月）**
   - 完成 L1/L2 初版建设
   - 选定 1–2 个代表性项目建立 L3
   - 收集并结构化导入一批典型缺陷到 L4

2. **第 2 阶段：RAG 服务与 API（1–2 个月）**
   - 按本文提供代码改造为企业实际服务
   - 建立知识导入、Chunking、向量化的 ETL 流程
   - 对接内部测试管理平台，实现“自动生成用例”的按钮

3. **第 3 阶段：推广与优化（2–6 个月）**
   - 在更多项目中推广使用
   - 收集反馈，优化 Prompt、检索策略、知识结构
   - 逐步引入更多场景：回归用例选取、风险分析、缺陷聚类等

通过上述方案，企业可以从“零散的测试经验 + 人工测试设计”升级到：

> **“有体系的测试知识库 + AI 驱动的测试工程”**

既保持测试质量的可控与标准化，又显著提升效率与可复用性，为构建现代化的软件质量体系打下坚实基础。

---

如果你愿意，我可以在此基础上，进一步为你：

- 针对某一特定行业（如金融/电商）定制 L2 示例文档与测试场景
- 针对你现有的技术栈（如内网大模型、私有化 OpenAI 等）给出更精确的 SDK 代码适配
- 设计一份“测试平台交互界面原型”，说明测试人员如何实际操作使用这一套能力