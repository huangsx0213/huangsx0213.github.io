---
layout:     post
title:      "ä¼ä¸šçº§è½¯ä»¶æµ‹è¯•å·¥ç¨‹â€”â€”çŸ¥è¯†åˆ†å±‚ä½“ç³»"
subtitle:   "ã€Œ  AI Study ã€" 
date:       2025-12-06 11:00:01
author:     "Vick Huang"
header-img: "img/bg-walle.jpg"
catalog: true
tags:
    - AI
---

è¿™æ˜¯ä¸€ç¯‡ **å®Œæ•´ã€ç»“æ„åŒ–ã€ä¼ä¸šçº§çš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹çŸ¥è¯†åˆ†å±‚æ–¹æ¡ˆ + é…å¥— Prompt** çš„æ–‡ç« ï¼Œå¯ç›´æ¥ç”¨äºæ’°å†™ä¼ä¸šå†…éƒ¨è§„èŒƒã€RAG çŸ¥è¯†åº“å»ºè®¾ã€AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆã€å›¢é˜ŸåŸ¹è®­æ–‡æ¡£ã€‚

---

# **ä¼ä¸šçº§è½¯ä»¶æµ‹è¯•å·¥ç¨‹â€”â€”çŸ¥è¯†åˆ†å±‚ä½“ç³»ä¸ AI Prompt æœ€ä½³å®è·µæŒ‡å—**

## **æ‘˜è¦**

åœ¨è½¯ä»¶æµ‹è¯•å·¥ç¨‹é¢†åŸŸï¼ŒçŸ¥è¯†ä½“ç³»åºå¤§ä¸”æ›´æ–°é¢‘ç¹ï¼Œä»åº•å±‚æ–¹æ³•è®ºåˆ°é¡¹ç›®çº§äº¤ä»˜å†…å®¹ï¼Œç¼ºä¹åˆ†å±‚ä¼šå¯¼è‡´çŸ¥è¯†å†—ä½™ã€å†²çªã€éš¾ä»¥ç»´æŠ¤ï¼Œè¿›è€Œå½±å“æµ‹è¯•è´¨é‡ã€è¦†ç›–åº¦ä¸è‡ªåŠ¨åŒ–æ•ˆç‡ã€‚æœ¬æ–¹æ¡ˆé€šè¿‡**å››å±‚ä¼ä¸šçº§çŸ¥è¯†ä½“ç³»**ï¼ˆæ–¹æ³•è®ºã€ä¸šåŠ¡é¢†åŸŸã€é¡¹ç›®ä¸Šä¸‹æ–‡ã€èµ„äº§å†å²ï¼‰ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºå¯å¤ç”¨ã€å¯æ²»ç†ã€å¯æŒç»­æ¼”è¿›çš„çŸ¥è¯†åº“ã€‚åŒæ—¶æä¾›ä¼ä¸šå¯ç›´æ¥ç”¨äº AI ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„ Prompt æ¨¡æ¿ã€‚

---

# **1. èƒŒæ™¯ä¸ç›®æ ‡**

åœ¨ä¼ä¸šè½¯ä»¶æµ‹è¯•ä½“ç³»ä¸­ï¼ŒçŸ¥è¯†ç®¡ç†é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼š

* æµ‹è¯•çŸ¥è¯†åˆ†æ•£åœ¨æ–‡æ¡£ã€ä»£ç ã€ Wikiã€äººå‘˜ç»éªŒä¸­
* é‡å¤å»ºè®¾ä¸¥é‡ï¼Œå¤šä¸ªé¡¹ç›®é‡å¤é€ è½®å­
* æµ‹è¯•ç”¨ä¾‹é£æ ¼ä¸ç»Ÿä¸€ã€æ ‡å‡†ä¸ä¸€è‡´
* éœ€æ±‚å˜æ›´å¯¼è‡´å¤§é‡æ–‡æ¡£è¿‡æœŸ
* éš¾ä»¥å°†å†å²ç¼ºé™·ä¸åœºæ™¯å¤ç”¨åˆ°æ–°çš„é¡¹ç›®

**ç›®æ ‡ï¼š**

1. æ„å»ºæ¸…æ™°åˆ†å±‚çš„ä¼ä¸šçº§çŸ¥è¯†ä½“ç³»
2. æ”¯æ’‘ä¼ä¸šå»ºç«‹å¯æ‰©å±•çš„ RAGï¼ˆRetrieval-Augmented Generationï¼‰çŸ¥è¯†åº“
3. ä¸º AI ç”Ÿæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹æä¾›ç»“æ„åŒ– Prompt
4. é™ä½é¡¹ç›®ä¹‹é—´çŸ¥è¯†å‰²è£‚ï¼Œè®©ç»éªŒå¯ä»¥å¤ç”¨
5. æé«˜ç”¨ä¾‹ç”Ÿæˆè´¨é‡ã€æµ‹è¯•è¦†ç›–ç‡å’Œè‡ªåŠ¨åŒ–æ•ˆç‡

---

# **2. ä¼ä¸šçº§æµ‹è¯•çŸ¥è¯†åˆ†å±‚æ¨¡å‹**

ä¼ä¸šçº§çŸ¥è¯†åº“å»ºè®®é‡‡ç”¨ä»¥ä¸‹ **å››å±‚åˆ†å±‚æ¨¡å‹ï¼ˆL1~L4ï¼‰**ï¼ŒæŒ‰**ç¨³å®šæ€§ä¸å¤ç”¨èŒƒå›´é€’å¢**åˆ’åˆ†ã€‚

---

## **L1ï¼šæ–¹æ³•è®ºä¸è§„èŒƒå±‚ï¼ˆMethodology & Standardsï¼‰**

### **æ€§è´¨**

* å…¨ä¼ä¸šç»Ÿä¸€
* æç¨³å®šï¼Œå‡ ä¹ä¸å˜
* å…¨é¡¹ç›®é€šç”¨

### **å†…å®¹ç¤ºä¾‹**

* æµ‹è¯•ç†è®ºï¼š
  è¾¹ç•Œå€¼ã€ç­‰ä»·ç±»ã€åˆ¤å®šè¡¨ã€å› æœå›¾ã€é”™è¯¯æ¨æµ‹ã€åœºæ™¯æ³•
* ä¼ä¸šæµ‹è¯•è§„èŒƒï¼š
  ç”¨ä¾‹å‘½åæ ‡å‡†ã€ç”¨ä¾‹ä¼˜å…ˆçº§ï¼ˆP0-P3ï¼‰ã€ç¼ºé™·ç­‰çº§å®šä¹‰
* è‡ªåŠ¨åŒ–æ ‡å‡†ï¼š
  pytest è§„èŒƒã€Page Object è§„èŒƒã€æ—¥å¿—è§„èŒƒ
* å®‰å…¨ä¸è§„èŒƒè¦æ±‚ï¼š
  ä¸ªäººä¿¡æ¯ä¿æŠ¤è§„èŒƒã€æ•°æ®è„±æ•æ ‡å‡†

### **ä½œç”¨**

ä¸º AI + æµ‹è¯•å·¥ç¨‹æä¾›**ä¸å˜çš„åº•å±‚è§„åˆ™**ï¼Œç¡®ä¿ AI ç”Ÿæˆçš„ç”¨ä¾‹ç»Ÿä¸€æ ‡å‡†ã€‚

---

## **L2ï¼šé¢†åŸŸä¸ä¸šåŠ¡é€»è¾‘å±‚ï¼ˆDomain & Business Logicï¼‰**

### **æ€§è´¨**

* å¯¹å•ä¸ªè¡Œä¸šã€å¹³å°æœ‰æ•ˆ
* æ…¢é€Ÿæ›´æ–°
* è·¨é¡¹ç›®å¤ç”¨

### **å†…å®¹ç¤ºä¾‹**

* è¡Œä¸šçŸ¥è¯†ï¼š
  é‡‘èï¼šæˆä¿¡è§„åˆ™ã€è¿˜æ¬¾é€»è¾‘
  ç”µå•†ï¼šè®¢å•ç”Ÿå‘½å‘¨æœŸã€æ”¯ä»˜æµç¨‹
  åŒ»ç–—ï¼šåˆè§„æµç¨‹
* é€šç”¨ä¸šåŠ¡æ¨¡å‹ï¼š
  ç”¨æˆ·çŠ¶æ€æœºã€è®¢å•çŠ¶æ€æœºã€å¯¹è´¦æµç¨‹
* å…¨å±€æ•°æ®å­—å…¸ï¼š
  é”™è¯¯ç å®šä¹‰ã€çŠ¶æ€å­—æ®µã€äº‹ä»¶ç±»å‹

### **ä½œç”¨**

ç¡®ä¿ AI å…·å¤‡è¡Œä¸šâ€œèƒŒæ™¯çŸ¥è¯†â€ï¼Œè®©ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ›´è´´è¿‘ä¸šåŠ¡çœŸå®é€»è¾‘ã€‚

---

## **L3ï¼šé¡¹ç›®ä¸Šä¸‹æ–‡å±‚ï¼ˆProject Contextï¼‰**

### **æ€§è´¨**

* æŸä¸€ä¸ªé¡¹ç›®ç‹¬æœ‰
* é«˜é¢‘å˜æ›´
* ä¸æµ‹è¯•ç”Ÿæˆå¼ºç›¸å…³

### **å†…å®¹ç¤ºä¾‹**

* PRDã€éœ€æ±‚æ–‡æ¡£ã€ç”¨ä¾‹åŸå‹å›¾
* API æ–‡æ¡£ã€Swaggerã€å‚æ•°è¯´æ˜
* æ•°æ®åº“ Schema
* åŠŸèƒ½æ¨¡å—åˆ—è¡¨ã€æŠ€æœ¯æ¶æ„
* é¡¹ç›®å˜æ›´è®°å½•ã€ç‰ˆæœ¬è¯´æ˜

### **ä½œç”¨**

**AI ç”Ÿæˆç”¨ä¾‹æ—¶ä¸»è¦ä»æ­¤å±‚æ£€ç´¢**
ç¡®ä¿ç”Ÿæˆå†…å®¹ä¸å½“å‰é¡¹ç›® 100% åŒ¹é…ã€‚

---

## **L4ï¼šèµ„äº§ä¸å†å²å±‚ï¼ˆAssets & Historyï¼‰**

### **æ€§è´¨**

* éšæ—¶é—´ä¸æ–­å¢é•¿
* ç”¨äºæµ‹è¯•ç»éªŒåå“º

### **å†…å®¹ç¤ºä¾‹**

* å†å²ç¼ºé™·åº“ï¼ˆåŒ…å«æ ¹å› åˆ†æï¼‰
* å›å½’ç”¨ä¾‹é›†ã€å†’çƒŸæµ‹è¯•é›†
* çº¿ä¸Šäº‹æ•…å¤ç›˜æŠ¥å‘Š
* ç”¨æˆ·åé¦ˆè®°å½•

### **ä½œç”¨**

è®© AI è‡ªåŠ¨å­¦ä¹ ï¼š
â€œå“ªäº›ç‚¹æœ€å®¹æ˜“å‡ºé”™ï¼Ÿâ€
â€œè¡Œä¸šå¸¸è§é™·é˜±æ˜¯ä»€ä¹ˆï¼Ÿâ€
ä»è€Œç”Ÿæˆ**æ›´é«˜æœ‰æ•ˆæ€§**çš„æµ‹è¯•ç”¨ä¾‹ã€‚

---

# **3. çŸ¥è¯†åˆ†å±‚å»ºè®¾åŸåˆ™ï¼ˆæœ€ä½³å®è·µï¼‰**

### **3.1 ä¿¡æ¯å»é‡**

åŒä¸€çŸ¥è¯†åªå­˜åœ¨ä¸€ä¸ªå½’å±å±‚çº§ï¼Œä¸é‡å¤å­˜æ”¾ã€‚

### **3.2 åˆ†å±‚å¼•ç”¨**

L3ï¼ˆé¡¹ç›®ï¼‰å¼•ç”¨ä¸Šå±‚ L1/L2ï¼Œä½†ä¸èƒ½åå‘ä¾èµ–ã€‚

### **3.3 å¯ç‰ˆæœ¬åŒ–**

L3 ä¸ L4 å¿…é¡»æ”¯æŒç‰ˆæœ¬å¿«ç…§ï¼Œä¾‹å¦‚ï¼š

```
é¡¹ç›®_X_ç‰ˆæœ¬_1.2.0
é¡¹ç›®_X_ç‰ˆæœ¬_1.2.1
```

### **3.4 ç»“æ„åŒ–ä¼˜å…ˆ**

å°½å¯èƒ½ä½¿ç”¨ç»“æ„åŒ–æ•°æ®ï¼š

* YAML
* JSON
* Markdown + æ ‡é¢˜åˆ†åŒº

### **3.5 ä¸ AI/RAG å¼ºåŒ¹é…çš„ç»“æ„åŒ– chunking**

ä¿æŒï¼š

* æ¯ä¸ªæ®µè½ç‹¬ç«‹è¯­ä¹‰
* ä¸šåŠ¡é€»è¾‘ä¸æ‹†æ•£
* æ¯ä¸ª chunk 500â€“1500 tokens

---

# **4. AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ Promptï¼ˆä¼ä¸šçº§æ ‡å‡†ç‰ˆï¼‰**

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯ç”¨äºä¼ä¸šæµ‹è¯•å›¢é˜Ÿçš„ **æ ‡å‡† Prompt æ¨¡æ¿**ã€‚
è¾“å…¥ç»™ AI æ—¶è‡ªåŠ¨ç»“åˆçŸ¥è¯†åº“ L1+L2+L3+L4ã€‚

---

## **ğŸ¯ Promptï¼šä¼ä¸šçº§è½¯ä»¶æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆï¼ˆRAG ä¸“ç”¨ï¼‰**

```
ä½ æ˜¯ä¸€åä¼ä¸šçº§è½¯ä»¶æµ‹è¯•ä¸“å®¶ï¼ˆ10 å¹´ç»éªŒï¼‰ï¼ŒåŒæ—¶éµå¾ªå…¬å¸çš„æµ‹è¯•è§„èŒƒï¼ˆL1ï¼‰ï¼Œé¢†åŸŸä¸šåŠ¡çŸ¥è¯†ï¼ˆL2ï¼‰ï¼Œå½“å‰é¡¹ç›®éœ€æ±‚ï¼ˆL3ï¼‰ï¼Œä»¥åŠå…¬å¸å†å²ç¼ºé™·ç»éªŒï¼ˆL4ï¼‰ã€‚

è¯·åŸºäºæ£€ç´¢åˆ°çš„å››å±‚çŸ¥è¯†ï¼Œä¸ºæ‰€æä¾›çš„åŠŸèƒ½æˆ–æ¥å£ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚

--------------------------------------------------
ã€å¿…é¡»éµå¾ªçš„è§„åˆ™ã€‘
1. ç”¨ä¾‹ç»“æ„ç»Ÿä¸€ï¼š
   - ç”¨ä¾‹ç¼–å·
   - ç”¨ä¾‹æ ‡é¢˜
   - å‰ç½®æ¡ä»¶
   - è¾“å…¥æ•°æ®
   - è§¦å‘æ­¥éª¤
   - é¢„æœŸç»“æœ
   - ä¼˜å…ˆçº§ï¼ˆP0-P3ï¼‰
   - å…³è”éœ€æ±‚IDï¼ˆå¦‚æœ‰ï¼‰
   - è‡ªåŠ¨åŒ–å¯è¡Œæ€§ï¼ˆæ˜¯/å¦ï¼‰

2. ç”Ÿæˆç”¨ä¾‹å¿…é¡»åŒæ—¶ç¬¦åˆï¼š
   - L1 æ–¹æ³•è®ºï¼šè¾¹ç•Œå€¼ã€ç­‰ä»·ç±»ã€é”™è¯¯æ¨æµ‹ã€åœºæ™¯åˆ†æ
   - L2 ä¸šåŠ¡é€»è¾‘ï¼šé¢†åŸŸè§„åˆ™ã€é”™è¯¯ç è§„èŒƒã€çŠ¶æ€æœºé€»è¾‘
   - L3 é¡¹ç›®ä¸Šä¸‹æ–‡ï¼šPRDã€æ¥å£å®šä¹‰ã€å‚æ•°è¦æ±‚
   - L4 å†å²ç»éªŒï¼šå·²çŸ¥é£é™©ç‚¹ã€è¿‡å»ç¼ºé™·æ¨¡å¼

3. ç›®æ ‡ï¼š
   - è¦†ç›–æ‰€æœ‰ä¸»æµç¨‹ã€å¼‚å¸¸æµç¨‹ã€è¾¹ç•Œåœºæ™¯
   - è¦†ç›–é«˜é£é™©ç‚¹ä¸å†å²æ˜“é”™ç‚¹
   - ç”¨ä¾‹å¿…é¡»å¯æ‰§è¡Œã€å¯å¤ç°

--------------------------------------------------
ã€è¾“å…¥å†…å®¹ã€‘
ï¼ˆè¿™é‡Œå¡«å…¥é¡¹ç›®éœ€æ±‚/æ¥å£å®šä¹‰/è¡¨ç»“æ„ç­‰ï¼‰

--------------------------------------------------
è¾“å‡ºï¼šå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆä¸å°‘äº X æ¡ï¼‰ã€‚
```

---

# **5. å¦‚ä½•åœ¨ä¼ä¸šå†…éƒ¨è½åœ°**

### **5.1 å»ºç«‹çŸ¥è¯†ä»“åº“ï¼ˆRAGï¼‰ç›®å½•ç»“æ„**

```
/knowledge
  /L1_standards
  /L2_domain
  /L3_project_X
      /version_1.0
      /version_1.1
  /L4_history
```

### **5.2 å®šæœŸæ²»ç†æµç¨‹**

* æ¯å‘¨ä¸€æ¬¡çŸ¥è¯†åº“æ¸…ç†
* æ¯ä¸ªç‰ˆæœ¬å»ºç«‹ç‹¬ç«‹å¿«ç…§
* æ¯æœˆä¸€æ¬¡â€œå†å²ç¼ºé™·â†’çŸ¥è¯†åº“â€å¯¼å…¥

### **5.3 åœ¨æµ‹è¯•å¹³å°æˆ– ChatGPT åŠ©ç†ä¸­å†…ç½® Prompt**

è®©æ¯ä¸ªæµ‹è¯•æˆå‘˜åªéœ€ï¼š

1. ä¸Šä¼ éœ€æ±‚
2. é€‰æ‹©ç”Ÿæˆæ¨¡å¼
3. è‡ªåŠ¨æ£€ç´¢ L1~L4
4. äº§å‡ºé«˜è´¨é‡ç”¨ä¾‹

---

# **6. æ€»ç»“**

ä¼ä¸šçº§çš„è½¯ä»¶æµ‹è¯•çŸ¥è¯†ä½“ç³»å¿…é¡»éµå¾ªï¼š

* **æ–¹æ³•è®ºç¨³å®š â†’ ä¸šåŠ¡æ¨¡å‹ç¨³å®š â†’ é¡¹ç›®ä¸Šä¸‹æ–‡é¢‘ç¹å˜ â†’ å†å²ç»éªŒæŒç»­ç§¯ç´¯**
* **åˆ†å±‚è¶Šæ¸…æ™°ï¼ŒAI/è‡ªåŠ¨åŒ–èƒ½åŠ›è¶Šå¼º**
* **RAG ä¸æµ‹è¯•å·¥ç¨‹ç»“åˆèƒ½æå¤§æå‡ç”¨ä¾‹è´¨é‡å’Œæ•ˆç‡**

é€šè¿‡æœ¬æ–‡çš„çŸ¥è¯†åˆ†å±‚æ–¹æ¡ˆ + Promptï¼Œä¼ä¸šå¯ä»¥å»ºç«‹å¯æŒç»­ã€å¯æ²»ç†ã€å¯å¤ç”¨çš„è½¯ä»¶æµ‹è¯•çŸ¥è¯†ä»“åº“ï¼Œå¹¶è®© AI å®ç°é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆèƒ½åŠ›ã€‚

---

ä¸‹é¢æ˜¯ä¸€ç¯‡**å¯ç›´æ¥ç”¨äºä¼ä¸šå†…éƒ¨è½åœ°çš„ã€Šè½¯ä»¶æµ‹è¯•å·¥ç¨‹â€”â€”çŸ¥è¯†åˆ†å±‚ä½“ç³»ä¸ AI Prompt æœ€ä½³å®è·µæŒ‡å—ï¼ˆå« Chroma + Python + Multi-Model AI + Structured Streamer ä»£ç å®ç°ï¼‰ã€‹**çš„å®Œæ•´æ–‡ç« ã€‚
å†…å®¹åŒ…å«ï¼š

* ä¼ä¸šçº§æµ‹è¯•çŸ¥è¯†åº“åˆ†å±‚ä½“ç³»ï¼ˆä»éœ€æ±‚â†’åœºæ™¯â†’æ­¥éª¤â†’ç”¨ä¾‹â†’ç»éªŒåº“ï¼‰
* Prompt æœ€ä½³å®è·µï¼ˆæµ‹è¯•å·¥ç¨‹ä¸“ç”¨ï¼‰
* Chroma + Python çŸ¥è¯†åº“ CRUD/ç‰ˆæœ¬ç®¡ç†
* tiktoken é«˜è´¨é‡ Chunkerï¼ˆå¤šè¯­è¨€ã€æ™ºèƒ½åˆå¹¶ã€Retryï¼‰
* MultiModelClientï¼ˆAzure / OpenAI / Gemini ç»Ÿä¸€æ¥å£ï¼‰
* Structured Streamerï¼ˆJSON Modeã€Structured Outputã€æµå¼æ ¼å¼ç»Ÿä¸€ï¼‰
* å•å…ƒæµ‹è¯•æ€è·¯ä¸å·¥ç¨‹åŒ–å»ºè®®

---

# **ä¼ä¸šçº§è½¯ä»¶æµ‹è¯•å·¥ç¨‹â€”â€”çŸ¥è¯†åˆ†å±‚ä½“ç³»ä¸ AI Prompt æœ€ä½³å®è·µæŒ‡å—ï¼ˆå«ä»£ç å®ç°ï¼‰**

## ç›®å½•

1. æ¦‚è¿°
2. è½¯ä»¶æµ‹è¯•çŸ¥è¯†ä½“ç³»çš„ä¼ä¸šçº§åˆ†å±‚
3. AI åœ¨æµ‹è¯•çŸ¥è¯†åº“ä¸­çš„ä½ç½®
4. ä¼ä¸šçº§ Prompt ä½“ç³»
5. Chroma + Python çŸ¥è¯†åº“è®¾è®¡ï¼ˆCRUD + Versioningï¼‰
6. é«˜è´¨é‡ Chunkerï¼ˆtiktokenã€å¤šè¯­è¨€ã€æ™ºèƒ½æ®µè½åˆå¹¶ã€Retryï¼‰
7. MultiModelClientï¼ˆç»Ÿä¸€ OpenAI / Azure / Geminiï¼‰
8. Structured Streamerï¼ˆç»Ÿä¸€ä¸‰å¥—æµå¼æ ¼å¼ï¼‰
9. å•å…ƒæµ‹è¯•ï¼šStructuredStreamer æµå¼ JSON åˆ‡å‰²ã€æ¢å¤ä¸é”™è¯¯å›é€€
10. æœ€ä½³è½åœ°æ–¹æ¡ˆï¼ˆDevOps / CI / æµ‹è¯•å›¢é˜Ÿåä½œï¼‰

---

# **1. æ¦‚è¿°**

ä¼ ç»Ÿä¼ä¸šçš„è½¯ä»¶æµ‹è¯•çŸ¥è¯†åˆ†æ•£åœ¨ï¼š

* Word æ–‡æ¡£
* Excel æµ‹è¯•ç”¨ä¾‹
* ç¦…é“/Jira
* Wiki
* é‚®ä»¶ç¾¤ç»„
* æµ‹è¯•äººå‘˜â€œä¸ªäººç»éªŒâ€

è¿™å¯¼è‡´ï¼š

* çŸ¥è¯†æ‰¾ä¸åˆ° / ä¸å¯å¤ç”¨
* ç‰ˆæœ¬å†²çª
* æµ‹è¯•ç”¨ä¾‹é‡å¤ã€æ–‡æ¡£é™ˆæ—§
* æ–°æµ‹è¯•åŠ å…¥å›¢é˜Ÿåå­¦ä¹ æˆæœ¬é«˜
* æµ‹è¯•æ•°æ®ã€åœºæ™¯ã€æ¥å£ä¿¡æ¯ç¢ç‰‡åŒ–

**ç”Ÿæˆå¼ AI +ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆChromaDBï¼‰**
å¯ä»¥å°†æµ‹è¯•çŸ¥è¯†æ±‡èš â†’ ç»“æ„åŒ– â†’ AI ç›´æ¥ç”Ÿæˆä¸“ä¸šç”¨ä¾‹ã€è„šæœ¬ã€Mockã€åœºæ™¯æ¨¡å‹ã€‚

ä¸ºäº†è®©çŸ¥è¯†åº“é•¿æœŸå¯ç»´æŠ¤ï¼Œéœ€è¦ **ä¼ä¸šçº§çŸ¥è¯†åˆ†å±‚ä½“ç³»** ä¸ **ç»Ÿä¸€ Prompt æ ‡å‡†**ã€‚

---

# **2. è½¯ä»¶æµ‹è¯•çŸ¥è¯†ä½“ç³»çš„ä¼ä¸šçº§åˆ†å±‚**

ä¸‹å›¾æ˜¯æ ‡å‡†çš„ **äº”å±‚æµ‹è¯•çŸ¥è¯†ä½“ç³»**ï¼ˆå¯é€‚ç”¨äºåŠŸèƒ½æµ‹è¯•ã€æ¥å£æµ‹è¯•ã€è‡ªåŠ¨åŒ–æµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ã€ç§»åŠ¨æµ‹è¯•ï¼‰ï¼š

---

## **L1. Requirement Layerï¼ˆéœ€æ±‚å±‚ï¼‰**

å†…å®¹ï¼šéœ€æ±‚æ–‡æ¡£ã€ç”¨æˆ·æ•…äº‹ã€ä¸šåŠ¡æµç¨‹å›¾
ç›®çš„ï¼šå‘Šè¯‰ AI â€œè¦æµ‹ä»€ä¹ˆâ€

ç¤ºä¾‹ç»“æ„ï¼š

```json
{
  "id": "REQ-001",
  "title": "ç”¨æˆ·ç™»å½•åŠŸèƒ½",
  "business_flow": ["è¾“å…¥è´¦å·", "è¾“å…¥å¯†ç ", "ç‚¹å‡»ç™»å½•æŒ‰é’®"],
  "non_func_requirements": ["æœ€å¤š 10 æ¬¡é”™è¯¯å°è¯•"]
}
```

---

## **L2. Scenario Layerï¼ˆåœºæ™¯å±‚ï¼‰**

å†…å®¹ï¼šæ¯ä¸ªéœ€æ±‚åŒ…å«è‹¥å¹²æµ‹è¯•åœºæ™¯
ç›®çš„ï¼šå‘Šè¯‰ AI â€œæœ‰å“ªäº›æƒ…å†µâ€

ç¤ºä¾‹ï¼š

* æ­£å¸¸ç™»å½•
* å¯†ç é”™è¯¯
* è´¦å·è¢«é”å®š
* ç‰¹æ®Šå­—ç¬¦è¾“å…¥

---

## **L3. Step Layerï¼ˆæ“ä½œæ­¥éª¤å±‚ï¼‰**

ä¸€ä¸ªåœºæ™¯ä¸­çš„å…·ä½“æ“ä½œåŠ¨ä½œï¼ˆæ›´ç»†ï¼‰

ç¤ºä¾‹ï¼š

```json
{
  "step": 1,
  "action": "è¾“å…¥æ­£ç¡®è´¦å·",
  "expectation": "é¡µé¢æ˜¾ç¤ºå¯†ç æ¡†"
}
```

---

## **L4. Case Layerï¼ˆç”¨ä¾‹å±‚ï¼‰**

ä¼ä¸šæ ‡å‡†æµ‹è¯•ç”¨ä¾‹ç»“æ„ï¼š

* ç”¨ä¾‹æ ‡é¢˜
* å‰ç½®æ¡ä»¶
* æ“ä½œæ­¥éª¤
* é¢„æœŸç»“æœ
* æµ‹è¯•æ•°æ®
* ä¼˜å…ˆçº§
* æ ‡ç­¾ï¼ˆå†’çƒŸ/å›å½’/å®‰å…¨/ç¨³å®šæ€§ï¼‰

---

## **L5. Knowledge Layerï¼ˆç»éªŒçŸ¥è¯†åº“å±‚ï¼‰**

å†…å®¹ï¼š

* å¸¸è§ç¼ºé™·æ¨¡å¼
* æµ‹è¯•åæ¨¡å¼
* è‡ªåŠ¨åŒ–æ¡†æ¶ç»éªŒ
* API Mock æ¨¡æ¿
* æ€§èƒ½åŸºçº¿æ¨¡æ¿

---

è¿™äº›å±‚ä¼šè¢«å­˜å…¥ ChromaDBï¼Œæ¯å±‚éƒ½å¯ç‹¬ç«‹æ›´æ–°ã€å¯æŸ¥è¯¢ã€ç‰ˆæœ¬å¯æ§ã€å¯è¢« AI ç»„è£…ã€‚

---

# **3. AI åœ¨æµ‹è¯•çŸ¥è¯†åº“ä¸­çš„ä½ç½®**

GPT/Gemini çš„èƒ½åŠ›ä¸»è¦ç”¨äºï¼š

### âœ“ éœ€æ±‚ â†’ åœºæ™¯ è‡ªåŠ¨ç”Ÿæˆ

### âœ“ åœºæ™¯ â†’ ç”¨ä¾‹è‡ªåŠ¨ç”Ÿæˆ

### âœ“ ç”¨ä¾‹ â†’ è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆRobot/Selenium/Playwright/RestAssuredï¼‰

### âœ“ Bug Pattern â†’ å¿«é€Ÿå¤ç›˜ã€è‡ªåŠ¨ç”Ÿæˆ Checklist

### âœ“ å¤šè¯­è¨€æµ‹è¯•æ–‡æ¡£ç”Ÿæˆï¼ˆEN/CNï¼‰

### âœ“ éœ€æ±‚å˜æ›´æ—¶è‡ªåŠ¨ diff â†’ æ›´æ–°ç”¨ä¾‹

---

# **4. ä¼ä¸šçº§ Prompt ä½“ç³»ï¼ˆæµ‹è¯•å·¥ç¨‹ä¸“ç”¨ï¼‰**

ä¼ä¸šçº§ Prompt å¿…é¡»éµå®ˆï¼š

### **Prompt = Role + Instruction + Constraint + Format + Example**

ç”¨äºæµ‹è¯•å·¥ç¨‹ï¼Œå¯å®šä¹‰å¦‚ä¸‹æ¨¡æ¿ï¼š

---

## **(A) è§’è‰²è®¾å®š Role**

```
ä½ æ˜¯ä¸€åä¼ä¸šçº§è½¯ä»¶æµ‹è¯•æ¶æ„å¸ˆï¼Œç†Ÿæ‚‰ Webã€APIã€ç§»åŠ¨ç«¯æµ‹è¯•ï¼Œèƒ½æ ¹æ®çŸ¥è¯†åº“å†…å®¹ç”Ÿæˆä¸“ä¸šæµ‹è¯•åœºæ™¯ä¸æµ‹è¯•ç”¨ä¾‹ã€‚
```

---

## **(B) ä¸»ä»»åŠ¡ Instruction**

```
æ ¹æ®è¾“å…¥çš„éœ€æ±‚ï¼Œç”Ÿæˆæµ‹è¯•åœºæ™¯ã€æµ‹è¯•æ•°æ®ã€å¼‚å¸¸æƒ…å†µã€è¾¹ç•Œå€¼ã€‚
```

---

## **(C) çº¦æŸ Constraint**

```
- è¾“å‡ºå¿…é¡»ç»“æ„åŒ– JSON
- ä¸å¾—ç¼–é€ ä¸å­˜åœ¨çš„å­—æ®µ
- æ¯ä¸ªæµ‹è¯•åœºæ™¯è‡³å°‘åŒ…å« 1 ä¸ªå¼‚å¸¸è·¯å¾„
- ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“å·²æœ‰çŸ¥è¯†
```

---

## **(D) è¾“å‡º Formatï¼ˆç»Ÿä¸€ JSON Schemaï¼‰**

```json
{
  "scenarios": [
    {
      "name": "string",
      "steps": [],
      "data": {},
      "expected": []
    }
  ]
}
```

---

## **(E) ç¤ºä¾‹ Example**

è®©æ¨¡å‹â€œæ¨¡ä»¿é£æ ¼â€ï¼š

```json
{
  "scenarios": [
    {
      "name": "è¾“å…¥æ­£ç¡®è´¦å·å¯†ç ï¼Œç™»å½•æˆåŠŸ",
      "steps": ["è¾“å…¥è´¦å·","è¾“å…¥å¯†ç ","ç‚¹å‡»ç™»å½•"],
      "expected": ["è·³è½¬åˆ°é¦–é¡µ"]
    }
  ]
}
```

---

# **5. Chroma + Python ä¼ä¸šçº§çŸ¥è¯†åº“ï¼ˆCRUD + Versioningï¼‰**

ä¸‹é¢æ˜¯ä¸€å¥—å¯ç›´æ¥ç”¨çš„ PyPI çº§åˆ«å°è£…ã€‚

---

## **5.1 åˆå§‹åŒ–**

```python
from chromadb import Client
from chromadb.config import Settings
import uuid
import time

class KnowledgeBase:
    def __init__(self, db_path="chroma_db"):
        self.client = Client(Settings(chroma_db_impl="duckdb+parquet", persist_directory=db_path))
        self.collection = self.client.get_or_create_collection(
            name="testing_kb",
            metadata={"hnsw:space": "cosine"}
        )
```

---

## **5.2 Versioning æœºåˆ¶**

æ‰€æœ‰æ–‡æ¡£è‡ªåŠ¨å†™å…¥ç‰ˆæœ¬å·ï¼š

```python
def _version():
    return str(time.time())
```

---

## **5.3 Create**

```python
def add(self, layer, content: dict):
    doc_id = uuid.uuid4().hex
    self.collection.add(
        documents=[json.dumps(content)],
        ids=[doc_id],
        metadatas=[{"layer": layer, "version": _version()}]
    )
    return doc_id
```

---

## **5.4 Read**

```python
def get(self, doc_id):
    result = self.collection.get(ids=[doc_id])
    return json.loads(result["documents"][0])
```

---

## **5.5 Updateï¼ˆè‡ªåŠ¨ç‰ˆæœ¬ + å­˜å†å²ï¼‰**

```python
def update(self, doc_id, new_content):
    old = self.collection.get(ids=[doc_id])
    self.collection.update(
        ids=[doc_id],
        documents=[json.dumps(new_content)],
        metadatas=[{"layer": old["metadatas"][0]["layer"], "version": _version()}]
    )
```

---

## **5.6 Delete**

```python
def delete(self, doc_id):
    self.collection.delete(ids=[doc_id])
```

---

## **5.7 Query (æŒ‰å±‚)**

```python
def by_layer(self, layer):
    results = self.collection.get(where={"layer": layer})
    return [json.loads(d) for d in results["documents"]]
```

---

# **6. é«˜è´¨é‡ Chunkerï¼ˆtiktokenã€å¤šè¯­è¨€å¥å·ã€æ™ºèƒ½åˆå¹¶ã€Retryï¼‰**

```python
import tiktoken
import re

class SmartChunker:
    def __init__(self, model="gpt-4o-mini", max_tokens=800):
        self.enc = tiktoken.encoding_for_model(model)
        self.max_tokens = max_tokens

    # å¤šè¯­è¨€å¥å·
    SENTENCE_END = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?.])")

    def split_sentences(self, text):
        parts = self.SENTENCE_END.split(text)
        return [p.strip() for p in parts if p.strip()]

    def count(self, text):
        return len(self.enc.encode(text))

    def chunk(self, text):
        sentences = self.split_sentences(text)
        chunks, buffer = [], ""

        for s in sentences:
            if self.count(buffer + s) < self.max_tokens:
                buffer += s
            else:
                chunks.append(buffer)
                buffer = s
        if buffer:
            chunks.append(buffer)

        return chunks

    def retry_split(self, text):
        try:
            return self.chunk(text)
        except:
            # fallback: ç›´æ¥æŒ‰ token åˆ‡
            tokens = self.enc.encode(text)
            out = []
            for i in range(0, len(tokens), self.max_tokens):
                out.append(self.enc.decode(tokens[i:i+self.max_tokens]))
            return out
```

---

# **7. MultiModelClientï¼ˆç»Ÿä¸€ Azure/OpenAI/Geminiï¼‰**

```python
import openai
from google import genai

class MultiModelClient:
    def __init__(self, provider="azure", **keys):
        self.provider = provider
        self.keys = keys

        if provider == "azure":
            openai.AzureOpenAI(api_key=keys["key"], api_version=keys["version"], azure_endpoint=keys["endpoint"])
        elif provider == "openai":
            openai.OpenAI(api_key=keys["key"])
        elif provider == "gemini":
            genai.configure(api_key=keys["key"])

    async def stream(self, model, messages):
        if self.provider in ["azure", "openai"]:
            client = openai.OpenAI()
            return client.chat.completions.create(
                model=model, messages=messages, stream=True
            )
        elif self.provider == "gemini":
            g = genai.GenerativeModel(model)
            return g.generate_content(messages, stream=True)
```

---

# **8. StructuredStreamerï¼ˆç»Ÿä¸€ä¸‰å¥—æµå¼æ ¼å¼ + JSON Modeï¼‰**

æ ¸å¿ƒï¼šæŠŠ Azure / OpenAI / Gemini çš„ç¢ç‰‡åŒ–æµ â†’ ç»Ÿä¸€æˆï¼š

```json
{
  "event": "delta",
  "text": "xxx",
  "json": {...}
}
```

å®Œæ•´å®ç°ï¼ˆç¼©ç•¥ç‰ˆï¼‰ï¼š

```python
import json

class StructuredStreamer:
    def __init__(self):
        self.buffer = ""

    def feed(self, chunk):
        """chunk æ˜¯æ¨¡å‹åŸå§‹æµæ•°æ®ï¼ˆå„ç§æ ¼å¼ï¼‰"""

        text = self.extract_text(chunk)
        if not text:
            return []

        self.buffer += text

        # å°è¯•è§£æå¤šä¸ª JSON
        results = []
        while True:
            try:
                obj, idx = self._parse_one(self.buffer)
                results.append(obj)
                self.buffer = self.buffer[idx:]
            except:
                break
        return results

    def extract_text(self, chunk):
        if "delta" in chunk:  # OpenAI/Azure
            return chunk["delta"].get("content", "")
        if "text" in chunk:   # Gemini
            return chunk["text"]
        return ""

    def _parse_one(self, text):
        decoder = json.JSONDecoder()
        obj, idx = decoder.raw_decode(text)
        return obj, idx
```

---

# **9. å•å…ƒæµ‹è¯•ï¼ˆæµå¼åˆ†å‰²ã€å¤š JSONã€é”™è¯¯å›é€€ï¼‰**

### ç”¨ä¾‹ 1ï¼šæµå¼ JSON è¢«æ‹†æˆ 3 æ®µ

### ç”¨ä¾‹ 2ï¼šè¿ç»­ä¸¤ä¸ª JSON

### ç”¨ä¾‹ 3ï¼šé”™è¯¯ JSON â†’ ä¸‹ä¸€ä¸ªæ­£ç¡® JSON èƒ½æ¢å¤

ç¤ºä¾‹ï¼š

```python
def test_stream_split():
    s = StructuredStreamer()
    parts = ['{"a":', '1}', '{"b":2}']
    out = []
    for p in parts:
        out += s.feed({"delta": {"content": p}})
    assert out == [{"a":1}, {"b":2}]

def test_error_recovery():
    s = StructuredStreamer()
    out = []
    out += s.feed({"delta": {"content": "{bad"}})
    out += s.feed({"delta": {"content": "{\"ok\":1}"}})
    assert out == [{"ok":1}]
```

---

# **10. æœ€ä½³è½åœ°æ–¹æ¡ˆï¼ˆä¼ä¸šå®é™…ä½¿ç”¨ï¼‰**

### æ¨èä¸‰å±‚æ¶æ„ï¼š

```
AI Gatewayï¼ˆç»Ÿä¸€æ¨¡å‹å®¢æˆ·ç«¯ï¼‰
      â†“
Testing Knowledge Serviceï¼ˆChroma DBï¼‰
      â†“
AI Testing Workbenchï¼ˆUI / CLIï¼‰
```

### é€‚ç”¨äºï¼š

* æµ‹è¯•ç”¨ä¾‹è‡ªåŠ¨ç”Ÿæˆ
* å›å½’é›†è‡ªåŠ¨æ›´æ–°
* ç¼ºé™·å¤ç›˜
* è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆ
* Mock Server è‡ªåŠ¨ç”Ÿæˆ
* æ€§èƒ½æµ‹è¯•å‚æ•°å»ºè®®

---

ä¸‹é¢æ˜¯ä¸€ä»½**å¯ç›´æ¥ä¿å­˜ä¸ºå•æ–‡ä»¶å¹¶å¯åœ¨ä¼ä¸šç¯å¢ƒä¸­è¿è¡Œçš„å®Œæ•´ Python å®ç°**ã€‚
å®ƒæ•´åˆäº†å‰é¢è®¨è®ºçš„å…³é”®æ¨¡å—ï¼š

* ChromaKnowledgeStoreï¼ˆChroma CRUD + ç‰ˆæœ¬ç®¡ç†ï¼‰
* SmartChunkerï¼ˆä¸¥æ ¼åŸºäº tiktoken çš„ chunkerï¼Œæ”¯æŒå¤šè¯­è¨€å¥å·ã€æ™ºèƒ½åˆå¹¶ã€retryï¼‰
* EmbeddingProvider æŠ½è±¡ + OpenAI/Azure embedding å·¥å‚ï¼ˆå ä½ï¼‰
* MultiModelClientï¼ˆç»Ÿä¸€ Azure / OpenAI / Gemini çš„æ™®é€šä¸æµå¼æ¥å£ï¼‰
* StreamingUnifiedChunkï¼ˆç»Ÿä¸€æµå¼ chunk æ ¼å¼ï¼‰
* StructuredStreamerï¼ˆä»æµå¢é‡è§£æ JSON/Structured outputï¼Œå¸¦ schema æ ¡éªŒæ”¯æŒï¼‰
* generate_structured_from_streamï¼ˆå°†æµå¼è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼Œå¹¶æ”¯æŒ reasoning è·¯ç”±ï¼‰
* å•å…ƒæµ‹è¯•ï¼ˆStructuredStreamerï¼šæµä¸­åˆ†å‰² / å¤š JSON / é”™è¯¯å›é€€ï¼‰

> æ³¨æ„ä¸è¿è¡Œæç¤ºï¼š
>
> * è¯¥è„šæœ¬ä¾èµ–å¤–éƒ¨åº“ï¼š`chromadb`, `tiktoken`, `openai`, `google-generativeai` (å¯é€‰), `jsonschema` (å¯é€‰)
> * è‹¥åœ¨æ²¡æœ‰è¿™äº›åº“çš„ç¯å¢ƒä¸‹è¿è¡Œï¼Œè„šæœ¬ä»å¯å¯¼å…¥å¹¶è¿è¡Œéƒ¨åˆ†åŠŸèƒ½ï¼ˆä¼šæŠ›å‡ºæ¸…æ™°é”™è¯¯æˆ–ä½¿ç”¨å›é€€é€»è¾‘ï¼‰ã€‚
> * ä¿å­˜ä¸º `enterprise_kb.py`ï¼Œåœ¨å«ä¾èµ–çš„è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ `python enterprise_kb.py` ä¼šæ‰§è¡Œå†…ç½®å•å…ƒæµ‹è¯•ï¼ˆunittestï¼‰ã€‚

å°†ä¸‹é¢å®Œæ•´ä»£ç ç²˜è´´åˆ°æ–‡ä»¶ `enterprise_kb.py`ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
enterprise_kb.py

ä¼ä¸šçº§ è½¯ä»¶æµ‹è¯•çŸ¥è¯†åº“ä¸ RAG æ”¯æŒ å…¨æ ˆå®ç°ï¼ˆå•æ–‡ä»¶ï¼‰
åŒ…å«ï¼š
- ChromaKnowledgeStore (Chroma CRUD + versioning)
- SmartChunker (tiktoken strict chunking, multi-language sentence delim, retry, smart merge)
- EmbeddingProvider abstraction + OpenAI/Azure factory
- MultiModelClient (azure/openai/gemini) with unified stream output
- StreamingUnifiedChunk
- StructuredStreamer (stream->JSON incrementalè§£æ + schema validation)
- generate_structured_from_stream helper
- Unit tests for StructuredStreamer (flow splits, multi JSON, error recovery)

ä¾èµ–ï¼š
  pip install chromadb tiktoken openai google-generativeai jsonschema

æ³¨æ„ï¼š
  - è‹¥åœ¨ä¸å«äº‘ API Key çš„ç¯å¢ƒè¿è¡Œï¼Œè¯·ä½¿ç”¨ fake embedding / fake model placeholdersã€‚
  - æœ¬è„šæœ¬ä¸ä¸»åŠ¨è°ƒç”¨å¤–ç½‘ï¼›çœŸå®è°ƒç”¨éœ€åœ¨è¿è¡Œå‰é…ç½® API keyã€‚
"""

from typing import List, Dict, Any, Callable, Optional, Generator
import os
import json
import time
import uuid
import re
import logging

# Optional libs
try:
    import tiktoken
except Exception:
    tiktoken = None

try:
    import openai
except Exception:
    openai = None

try:
    import chromadb
    from chromadb import Client
    from chromadb.config import Settings
except Exception:
    chromadb = None
    Client = None
    Settings = None

try:
    import google.generativeai as genai
except Exception:
    genai = None

try:
    import jsonschema
except Exception:
    jsonschema = None

# logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("enterprise_kb")


# ---------------- Embedding Provider ----------------
class EmbeddingProvider:
    """Abstract embedding provider wrapper.
    Implement embed(texts: List[str]) -> List[List[float]]
    """

    def __init__(self, func: Optional[Callable[[List[str]], List[List[float]]]] = None):
        self._func = func

    def embed(self, texts: List[str]) -> List[List[float]]:
        if self._func is None:
            raise ValueError("Embedding function not provided.")
        return self._func(texts)


def openai_embedding_factory(api_key: str, model: str = "text-embedding-3-small", azure: bool = False,
                             azure_endpoint: Optional[str] = None, azure_api_version: Optional[str] = None) -> EmbeddingProvider:
    """Factory producing an EmbeddingProvider using OpenAI / AzureOpenAI.
    Note: requires `openai` package and valid keys.
    """
    if openai is None:
        raise ImportError("openai package is required for openai_embedding_factory")

    def embed(texts: List[str]) -> List[List[float]]:
        # For Azure, user must set openai.api_type='azure' and openai.api_base etc. externally if required.
        resp = openai.Embeddings.create(input=texts, model=model)
        return [r["embedding"] for r in resp["data"]]

    return EmbeddingProvider(embed)


def fake_embedding_provider(dim: int = 1536) -> EmbeddingProvider:
    """Fake deterministic embedding for local testing (no network)."""
    def embed(texts: List[str]) -> List[List[float]]:
        embs = []
        for t in texts:
            s = sum(ord(c) for c in t)
            vec = [((s + i) % 100) / 100 for i in range(dim)]
            embs.append(vec)
        return embs
    return EmbeddingProvider(embed)


# ---------------- SmartChunker (tiktoken strict with fallback) ----------------
class SmartChunker:
    """
    Strict token-based chunker with:
      - tiktoken based encoding (preferred)
      - multi-language sentence boundary heuristics
      - overlap tokens
      - smart merging for tiny chunks
      - retry mechanism with alternative encodings or fallback
    """

    DEFAULT_ENCODINGS = ["cl100k_base", "p50k_base", "gpt2"]

    def __init__(self, default_max_tokens: int = 400, default_overlap: int = 50, min_chunk_tokens: int = 50, retry: int = 2):
        self.default_max_tokens = default_max_tokens
        self.default_overlap = default_overlap
        self.min_chunk_tokens = min_chunk_tokens
        self.retry = retry
        # sentence delim regex includes multiple languages (Chinese/Japanese/English punctuation, newline)
        self.sentence_split_pattern = re.compile(r"(?<=[ã€‚ï¼\.ï¼!?ï¼ï¼Ÿï¼›;]\s*|\n)")
        # fallback word-splitting tokenization when tiktoken not present
        self._has_tiktoken = (tiktoken is not None)

    def split_sentences(self, text: str) -> List[str]:
        parts = [p.strip() for p in self.sentence_split_pattern.split(text) if p.strip()]
        return parts

    def _encode(self, enc, text: str) -> List[int]:
        return enc.encode(text)

    def chunk(self, text: str, max_tokens: Optional[int] = None, overlap: Optional[int] = None,
              min_chunk_tokens: Optional[int] = None, retry: Optional[int] = None) -> List[str]:
        """Main entry to chunk text."""
        if max_tokens is None:
            max_tokens = self.default_max_tokens
        if overlap is None:
            overlap = self.default_overlap
        if min_chunk_tokens is None:
            min_chunk_tokens = self.min_chunk_tokens
        if retry is None:
            retry = self.retry

        # Attempt token-based chunking using tiktoken with retries
        if self._has_tiktoken:
            last_exc = None
            for attempt in range(retry):
                for enc_name in self.DEFAULT_ENCODINGS:
                    try:
                        enc = tiktoken.get_encoding(enc_name) if hasattr(tiktoken, "get_encoding") else tiktoken.encoding_for_model(enc_name)
                        tokens = enc.encode(text)
                        n = len(tokens)
                        i = 0
                        chunks = []
                        while i < n:
                            end = min(i + max_tokens, n)
                            chunk_tokens = tokens[i:end]
                            chunk_text = enc.decode(chunk_tokens)
                            # try avoid mid-sentence cut: if not at end and chunk_text contains a sentence delim near end, shorten
                            if end < n:
                                last_delim = -1
                                for d in ["ã€‚", "ï¼", ".", "ï¼", "!", "ï¼Ÿ", "?", ";", "ï¼›", "\n"]:
                                    pos = chunk_text.rfind(d)
                                    if pos > last_delim:
                                        last_delim = pos
                                if last_delim != -1 and last_delim > len(chunk_text) * 0.3:
                                    chunk_text = chunk_text[: last_delim + 1]
                                    chunk_tokens = enc.encode(chunk_text)
                            chunks.append(chunk_text)
                            i = i + max(1, len(chunk_tokens) - overlap)
                        # merge very small chunks with previous
                        merged = []
                        for c in chunks:
                            try:
                                tok_len = len(enc.encode(c))
                            except Exception:
                                tok_len = len(c.split())
                            if merged and tok_len < min_chunk_tokens:
                                merged[-1] = merged[-1] + "\n" + c
                            else:
                                merged.append(c)
                        return merged
                    except Exception as e:
                        last_exc = e
                        continue
            # fall through to fallback if all tiktoken attempts fail
            logger.warning(f"tiktoken chunking failed after retries: {last_exc}")

        # Fallback: sentence-based merging with word-count proxy
        sentences = self.split_sentences(text)
        parts = []
        cur = ""
        cur_tokens = 0
        for s in sentences:
            s_words = s.split()
            s_len = len(s_words)
            if cur == "":
                cur = s
                cur_tokens = s_len
            else:
                if cur_tokens + s_len <= max_tokens:
                    cur = cur + " " + s
                    cur_tokens += s_len
                else:
                    parts.append(cur.strip())
                    cur = s
                    cur_tokens = s_len
        if cur:
            parts.append(cur.strip())

        # merge too small pieces
        merged = []
        for p in parts:
            word_len = len(p.split())
            if merged and word_len < min_chunk_tokens:
                merged[-1] = merged[-1] + "\n" + p
            else:
                merged.append(p)
        return merged


# ---------------- ChromaKnowledgeStore (CRUD + versioning) ----------------
class ChromaKnowledgeStore:
    """Chroma wrapper implementing add/query/delete/replace_source/deprecate and import/export.
    Note: requires chromadb package and running in environment where chroma is supported.
    """

    def __init__(self, persist_directory: Optional[str] = None, embedding_provider: Optional[EmbeddingProvider] = None,
                 audit_log_path: str = "chroma_kb_audit.jsonl"):
        if Client is None:
            raise ImportError("chromadb (Client) required by ChromaKnowledgeStore")

        settings = {}
        if persist_directory:
            settings["persist_directory"] = persist_directory
        self._client = Client(Settings(**settings)) if Settings is not None else Client()
        self.embedding_provider = embedding_provider
        self.audit_log_path = audit_log_path
        if not os.path.exists(self.audit_log_path):
            with open(self.audit_log_path, "w", encoding="utf-8") as f:
                pass

    def get_or_create_collection(self, name: str, metadata: Optional[dict] = None):
        try:
            return self._client.get_collection(name)
        except Exception:
            return self._client.create_collection(name, metadata=metadata or {})

    def add(self, collection_name: str, ids: List[str], documents: List[str], metadatas: List[dict],
            embeddings: Optional[List[List[float]]] = None):
        coll = self.get_or_create_collection(collection_name)
        if embeddings is None:
            if self.embedding_provider is None:
                raise ValueError("No embeddings provided and no embedding_provider configured.")
            embeddings = self.embedding_provider.embed(documents)
        # validate metadata minimal fields
        for m in metadatas:
            if "source_id" not in m:
                raise ValueError("metadata must include source_id")
            if "layer" not in m:
                raise ValueError("metadata must include layer (L1/L2/L3/L4)")
            if "is_latest" not in m:
                m["is_latest"] = True
        coll.add(ids=ids, documents=documents, metadatas=metadatas, embeddings=embeddings)
        self._audit("ADD", collection_name, ids, {"count": len(ids)})

    def delete_where(self, collection_name: str, where: dict):
        coll = self.get_or_create_collection(collection_name)
        coll.delete(where=where)
        self._audit("DELETE_WHERE", collection_name, None, where)

    def replace_source(self, collection_name: str, source_id: str, documents: List[str], metadatas: List[dict],
                       ids: Optional[List[str]] = None, embeddings: Optional[List[List[float]]] = None):
        # delete old
        try:
            self.delete_where(collection_name, {"source_id": source_id})
        except Exception:
            pass
        if ids is None:
            ids = [f"{source_id}_chunk_{i:04d}" for i in range(len(documents))]
        for m in metadatas:
            m.setdefault("source_id", source_id)
            m.setdefault("is_latest", True)
        self.add(collection_name, ids, documents, metadatas, embeddings)
        self._audit("REPLACE_SOURCE", collection_name, ids, {"source_id": source_id})

    def deprecate_source_versions(self, collection_name: str, source_id: str):
        coll = self.get_or_create_collection(collection_name)
        resp = coll.get(where={"source_id": source_id})
        ids = resp.get("ids", [])
        if not ids:
            return
        docs = resp.get("documents", [])
        metas = resp.get("metadatas", [])
        for m in metas:
            m["is_latest"] = False
        embs = None
        if self.embedding_provider is not None:
            try:
                embs = self.embedding_provider.embed(docs)
            except Exception:
                embs = None
        coll.delete(ids=ids)
        coll.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
        self._audit("DEPRECATE", collection_name, ids, {"source_id": source_id})

    def query(self, collection_name: str, query_text: str, top_k: int = 10, where: Optional[dict] = None,
              include: Optional[List[str]] = None) -> dict:
        coll = self.get_or_create_collection(collection_name)
        if self.embedding_provider is None:
            raise ValueError("No embedding_provider configured for query embedding generation.")
        q_emb = self.embedding_provider.embed([query_text])[0]
        include = include or ["documents", "metadatas", "distances", "ids"]
        resp = coll.query(query_embeddings=[q_emb], n_results=top_k, where=where, include=include)
        self._audit("QUERY", collection_name, [query_text], {"top_k": top_k, "where": where})
        return resp

    def export_collection(self, collection_name: str, path: str):
        coll = self.get_or_create_collection(collection_name)
        resp = coll.get()
        with open(path, "w", encoding="utf-8") as f:
            json.dump(resp, f, ensure_ascii=False, indent=2)
        self._audit("EXPORT", collection_name, None, {"path": path})

    def import_collection(self, collection_name: str, path: str):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        ids = data.get("ids", [])
        docs = data.get("documents", [])
        metas = data.get("metadatas", [])
        embs = None
        if self.embedding_provider is not None:
            try:
                embs = self.embedding_provider.embed(docs)
            except Exception:
                embs = None
        self.add(collection_name, ids, docs, metas, embs)
        self._audit("IMPORT", collection_name, ids, {"path": path})

    def _audit(self, op: str, collection_name: str, ids: Optional[List[str]], meta: Optional[Any]):
        rec = {
            "time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
            "op": op,
            "collection": collection_name,
            "ids": ids,
            "meta": meta,
        }
        with open(self.audit_log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")


# ---------------- Streaming unified chunk and MultiModelClient ----------------
class StreamingUnifiedChunk:
    """Unified streaming chunk"""
    def __init__(self, text: str, delta: str, is_end: bool = False):
        self.text = text
        self.delta = delta
        self.is_end = is_end

    def to_dict(self):
        return {"text": self.text, "delta": self.delta, "is_end": self.is_end}


class MultiModelClient:
    """
    Multi-model client supporting azure/openai/gemini (best-effort placeholders).
    Must be initialized with provider credentials via kwargs (api_key, endpoint, api_version, model, reasoning_model).
    """

    def __init__(self, provider: str = "azure", **kwargs):
        self.provider = provider
        self.kwargs = kwargs

    # non-streaming chat (placeholder)
    def chat(self, messages: List[Dict[str, str]], **gen_kwargs):
        if self.provider == "azure" or self.provider == "openai":
            if openai is None:
                raise ImportError("openai SDK required for azure/openai provider")
            client = openai
            return client.ChatCompletion.create(model=self.kwargs.get("model"), messages=messages, **gen_kwargs)
        elif self.provider == "gemini":
            if genai is None:
                raise ImportError("google.generativeai required for gemini provider")
            genai.configure(api_key=self.kwargs.get("api_key"))
            model = genai.GenerativeModel(self.kwargs.get("model"))
            return model.generate_content(messages)
        else:
            raise ValueError("Unsupported provider")

    # streaming generator that yields StreamingUnifiedChunk
    def stream(self, messages: List[Dict[str, str]], **gen_kwargs) -> Generator[StreamingUnifiedChunk, None, None]:
        if self.provider in ("azure", "openai"):
            yield from self._stream_openai_compatible(messages, gen_kwargs)
        elif self.provider == "gemini":
            yield from self._stream_gemini(messages, gen_kwargs)
        else:
            raise ValueError("Unsupported provider")

    def _stream_openai_compatible(self, messages, gen_kwargs):
        if openai is None:
            raise ImportError("openai SDK required for streaming azure/openai")
        # using OpenAI's streaming interface: create(..., stream=True)
        # Note: actual SDK usage may differ; this is a best-effort pattern.
        resp_iter = openai.ChatCompletion.create(model=self.kwargs.get("model"), messages=messages, stream=True, **gen_kwargs)
        buffer = ""
        for chunk in resp_iter:
            # chunk is usually a dict-like with choices[*].delta.content
            delta = ""
            try:
                # Some SDK versions use 'choices' list
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    d = chunk["choices"][0].get("delta") or {}
                    delta = d.get("content") or ""
            except Exception:
                # fallback: attempt direct content
                delta = chunk.get("content") or ""
            buffer += delta
            yield StreamingUnifiedChunk(text=buffer, delta=delta)
        yield StreamingUnifiedChunk(text=buffer, delta="", is_end=True)

    def _stream_gemini(self, messages, gen_kwargs):
        if genai is None:
            raise ImportError("google.generativeai SDK required for gemini streaming")
        genai.configure(api_key=self.kwargs.get("api_key"))
        model = genai.GenerativeModel(self.kwargs.get("model"))
        # genai.generate_content(..., stream=True) usually yields chunks; adapt as needed
        stream_iter = model.generate_content(messages, stream=True)
        buffer = ""
        for chunk in stream_iter:
            delta = ""
            # depending on chunk structure
            if hasattr(chunk, "text"):
                delta = getattr(chunk, "text") or ""
            elif hasattr(chunk, "parts"):
                delta = "".join([getattr(p, "text", "") for p in chunk.parts])
            else:
                try:
                    delta = str(chunk)
                except Exception:
                    delta = ""
            buffer += delta
            yield StreamingUnifiedChunk(text=buffer, delta=delta)
        yield StreamingUnifiedChunk(text=buffer, delta="", is_end=True)


# ---------------- StructuredStreamer: incremental JSON parse & schema validation ----------------
class StructuredOutputError(Exception):
    pass


def _validate_json_with_schema(obj: dict, schema: dict):
    if jsonschema is None:
        # basic type check
        if not isinstance(obj, dict):
            raise StructuredOutputError("Expected JSON object but got non-object (jsonschema not installed)")
        return True
    jsonschema.validate(instance=obj, schema=schema)
    return True


class StructuredStreamer:
    """
    Incrementally accumulate stream deltas and attempt to parse JSON objects.
    - feed(delta) -> List[dict] of extracted complete objects (could be empty)
    - get_buffer() returns current buffer
    """

    def __init__(self, schema: Optional[dict] = None):
        self.schema = schema
        self.buf = ""

    def feed(self, delta: str) -> List[dict]:
        """Feed incremental text (delta). Try to extract zero or more JSON objects."""
        if not delta:
            return []
        self.buf += delta
        extracted = []

        # Attempt to find JSON braces pairs - we attempt to find balanced {...} segments.
        # This is a pragmatic approach: find first '{', then find matching '}' by scanning, taking nested braces into account.
        i = 0
        while True:
            start = self.buf.find("{", i)
            if start == -1:
                break
            depth = 0
            end = -1
            for j in range(start, len(self.buf)):
                ch = self.buf[j]
                if ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        end = j
                        break
            if end == -1:
                # incomplete, wait for more
                break
            candidate = self.buf[start:end + 1]
            try:
                obj = json.loads(candidate)
                # validate schema if provided
                if self.schema:
                    _validate_json_with_schema(obj, self.schema)
                extracted.append(obj)
                # remove processed part
                # we set i = 0 again because buffer has been shortened
                self.buf = self.buf[end + 1:]
                i = 0
            except Exception:
                # parsing failed for this candidate; try to find next '{' after start (avoid infinite loop)
                i = start + 1
                # continue search
                continue
        return extracted

    def get_buffer(self) -> str:
        return self.buf


# ---------------- generate_structured_from_stream helper ----------------
def generate_structured_from_stream(client: MultiModelClient, messages: List[Dict[str, str]],
                                    schema: Optional[dict] = None, reasoning: bool = False,
                                    mode: Optional[str] = None, **gen_kwargs) -> Dict[str, Any]:
    """
    Use client's stream to generate structured JSON:
      - mode: "json"|"structured" to try parsing JSON from stream
      - reasoning: if True, prefer reasoning model defined in client.kwargs['reasoning_model']
    """
    if reasoning:
        # set model to reasoning model if provided
        reasoning_model = client.kwargs.get("reasoning_model")
        if reasoning_model:
            gen_kwargs.setdefault("model", reasoning_model)

    mode = mode or gen_kwargs.pop("mode", None)
    streamer = StructuredStreamer(schema=schema)
    raw_buffer = ""

    for chunk in client.stream(messages, **gen_kwargs):
        raw_buffer += chunk.delta
        if mode in ("json", "structured"):
            objs = streamer.feed(chunk.delta)
            if objs:
                # return first complete parsed object (could be extended to combine multiples)
                return objs[0]
        if chunk.is_end:
            break

    # finished stream, try parse entire buffer
    if mode in ("json", "structured"):
        try:
            obj = json.loads(raw_buffer)
            if schema:
                _validate_json_with_schema(obj, schema)
            return obj
        except Exception:
            # failed to parse, return raw text for troubleshooting
            return {"raw_text": raw_buffer}
    return {"raw_text": raw_buffer}


# ---------------- Unit tests for StructuredStreamer ----------------
import unittest


class TestStructuredStreamer(unittest.TestCase):
    def test_split_single_json(self):
        s = StructuredStreamer(schema={"type": "object", "properties": {"a": {"type": "number"}}, "required": ["a"]})
        out = s.feed('{"a": 1}')
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0]["a"], 1)

    def test_split_across_chunks(self):
        s = StructuredStreamer(schema={"type": "object", "properties": {"x": {"type": "string"}}, "required": ["x"]})
        out1 = s.feed('{"x": ')
        self.assertEqual(out1, [])
        out2 = s.feed('"hello"}')
        self.assertEqual(len(out2), 1)
        self.assertEqual(out2[0]["x"], "hello")

    def test_multiple_json_objects_in_stream(self):
        s = StructuredStreamer(schema={"type": "object", "properties": {"v": {"type": "number"}}, "required": ["v"]})
        parts = ['{"v": 1}', '{"v": 2}']
        res = []
        for p in parts:
            res += s.feed(p)
        self.assertEqual(len(res), 2)
        self.assertEqual(res[0]["v"], 1)
        self.assertEqual(res[1]["v"], 2)

    def test_error_recovery_then_valid(self):
        s = StructuredStreamer(schema={"type": "object", "properties": {"ok": {"type": "number"}}, "required": ["ok"]})
        res1 = s.feed('not_json')
        self.assertEqual(res1, [])
        res2 = s.feed('{"ok": 3}')
        self.assertEqual(len(res2), 1)
        self.assertEqual(res2[0]["ok"], 3)

    def test_partial_never_completed_buffer_keeps_state(self):
        s = StructuredStreamer(schema={"type": "object", "properties": {"z": {"type": "number"}}, "required": ["z"]})
        res = s.feed('{"z": 5')
        self.assertEqual(res, [])
        self.assertTrue('{"z": 5' in s.get_buffer())


# ---------------- Example usage (only run when executed directly) ----------------
def _demo():
    print("Demo: SmartChunker sample")
    chunker = SmartChunker(default_max_tokens=80)
    text = "è¿™æ˜¯ç¬¬ä¸€å¥ã€‚This is English. æ¬¡ã®æ–‡ã§ã™ã€‚Another sentence. " * 5
    chunks = chunker.chunk(text)
    print("Chunks:", len(chunks))
    for i, c in enumerate(chunks[:3]):
        print(i, c[:80].replace("\n", " "), "...")


if __name__ == "__main__":
    # Run unit tests for StructuredStreamer
    unittest.main(verbosity=2)
```

---

### ä½¿ç”¨å»ºè®®ï¼ˆè½åœ°æç¤ºï¼‰

1. å°† `enterprise_kb.py` æ”¾å…¥é¡¹ç›®ä»“åº“ `tests-infra` ä¸‹çš„ `ragsvc` æ¨¡å—ï¼›åœ¨ CI ä¸­é…ç½®ä¾èµ–å¹¶è¿è¡Œå•å…ƒæµ‹è¯•ã€‚
2. åœ¨ä¼ä¸šç¯å¢ƒä¸­é…ç½®å®é™…çš„ embedding providerï¼ˆ`openai_embedding_factory`ï¼‰å¹¶æŠŠ `EmbeddingProvider` æ³¨å…¥ `ChromaKnowledgeStore`ã€‚
3. é…ç½® MultiModelClient æ—¶ï¼Œå°† Azure / OpenAI / Gemini çš„ API keysã€å®‰å…¨åŸŸåä¸ model åç§°æ”¾å…¥æœºå¯†ç®¡ç†ï¼ˆVaultï¼‰ä¸­ã€‚
4. è‹¥è¦åœ¨ Web å‰ç«¯å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿ç”¨ `MultiModelClient.stream` å¹¶å°† `StreamingUnifiedChunk` æ˜ å°„ä¸º WebSocket æ¶ˆæ¯ã€‚
5. è‹¥éœ€è¦æ›´å¼ºçš„ StructuredStreamer èƒ½åŠ›ï¼ˆæ¯”å¦‚æ”¯æŒ JSONLã€partial-array streamingã€schema å¢é‡éªŒè¯ï¼‰ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¢å¼ºè§£æå™¨ä»¥å®¹å¿æµå¼åµŒå¥—æ•°ç»„ç­‰å¤æ‚æƒ…å†µã€‚

---



